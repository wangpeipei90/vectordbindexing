{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "from io_utils import read_fbin, read_ibin\n",
    "import faiss\n",
    "print(faiss.__version__)\n",
    "import numpy as np\n",
    "file_path = \"/root/code/vectordbindexing/Text2Image/base.1M.fbin\"\n",
    "query_path = \"/root/code/vectordbindexing/Text2Image/query.public.100K.fbin\"\n",
    "ground_truth_path = \"/root/code/vectordbindexing/Text2Image/groundtruth.public.100K.ibin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "reading image vector: ---\n",
      "<class 'numpy.ndarray'>\n",
      "2 (1000000, 200) float32 200000000\n",
      "\n",
      "\n",
      "reading querys: ---\n",
      "<class 'numpy.ndarray'>\n",
      "2 (100000, 200) float32 20000000\n"
     ]
    }
   ],
   "source": [
    "# read datasets\n",
    "print(\"\\n\\nreading image vector: ---\")\n",
    "data_vector = read_fbin(file_path)\n",
    "print(type(data_vector))\n",
    "print(data_vector.ndim, data_vector.shape, data_vector.dtype, data_vector.size)\n",
    "# print(data_vector[:1])  # Print first 1 elements to verify content\n",
    "\n",
    "train_data_vector = data_vector[:500000]\n",
    "insert_1_percent = data_vector[500000:505000]\n",
    "insert_2_percent = data_vector[505000:510000]\n",
    "insert_3_percent = data_vector[510000:515000]\n",
    "insert_4_percent = data_vector[515000:520000]\n",
    "insert_5_percent = data_vector[520000:525000]\n",
    "insert_10_percent = data_vector[525000:550000]\n",
    "\n",
    "# read querys\n",
    "print(\"\\n\\nreading querys: ---\")\n",
    "query_vector = read_fbin(query_path)\n",
    "print(type(query_vector))\n",
    "print(query_vector.ndim, query_vector.shape, query_vector.dtype, query_vector.size)\n",
    "# print(query_vector[0])  # Print first 3 elements to verify content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hnsw_cosine_status as hnsw_cosine\n",
    "import simple_sim_hash\n",
    "import importlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "importlib.reload(hnsw_cosine)\n",
    "\n",
    "# M=64 比较合适，甚至更宽的宽度\n",
    "# 这里是个经验值：会在增加宽度的同时，逐渐达到一个稳定值\n",
    "index = hnsw_cosine.HNSWIndex(M=32, ef_construction=128, ef_search=64, random_seed=1)\n",
    "simHash = simple_sim_hash.SimpleSimHash(dim=200)\n",
    "\n",
    "IMAGE_IDX_SET = set()\n",
    "\n",
    "# 形状 [N,200]（先用1M子集或更小切片做原型）\n",
    "for img_id, vec in enumerate(train_data_vector):        # 可加 tqdm、批量 flush\n",
    "    index.add_item_fast10k(vec, lsh=simHash, limit=100)\n",
    "    IMAGE_IDX_SET.add(img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train: 83333\n",
      "num of test: 16667\n"
     ]
    }
   ],
   "source": [
    "# 读取faiss搜索结果，获取 query_vector 和 search 结果\n",
    "import json\n",
    "train_query_list = {}\n",
    "test_query_list = {}\n",
    "\n",
    "# ground_truth = read_ibin(ground_truth_path)\n",
    "# print(type(ground_truth))\n",
    "# print(ground_truth.ndim, ground_truth.shape, ground_truth.dtype, ground_truth.size)\n",
    "# for query_idx in range(ground_truth.shape[0]):\n",
    "#     actual_groundtruth = []\n",
    "#     for idx in ground_truth[query_idx]:\n",
    "#         if idx > 500000:\n",
    "#             continue\n",
    "#         actual_groundtruth.append(idx)\n",
    "#     if len(actual_groundtruth) < 1:\n",
    "#         continue\n",
    "#     if int(query_idx) % 6 != 0:\n",
    "#         train_query_list[query_idx] = ground_truth[query_idx]\n",
    "#     else:\n",
    "#         test_query_list[query_idx] = ground_truth[query_idx]\n",
    "\n",
    "with open(\"./TempResults/search_results_100K.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    for query_idx, vec_list in data.items():\n",
    "        mList = []\n",
    "        for x in vec_list:\n",
    "            mList.append(x - int(query_idx))\n",
    "        if int(query_idx) % 6 != 0:\n",
    "            train_query_list[int(query_idx)] = mList\n",
    "        else:\n",
    "            test_query_list[int(query_idx)] = mList\n",
    "print(f\"num of train: {len(train_query_list)}\")\n",
    "print(f\"num of test: {len(test_query_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 阶段分析统计 ===\n",
      "第一阶段 (快速靠近) - 平均步数: 268.97, 平均加速边: 0.00\n",
      "第二阶段 (Beam Search) - 平均步数: 218.23, 平均加速边: 0.00\n",
      "整体加速边使用比例: 0.00%\n",
      "\n",
      "加速边受益最多的查询:\n",
      "  第一阶段: 293 步, 0 条加速边\n",
      "  第二阶段: 369 步, 0 条加速边\n",
      "  总步数: 663, 总加速边: 0\n",
      "  加速边比例: 0.00%\n",
      "\n",
      "原始搜索统计:\n",
      "mean steps: 488.1981869484301\n",
      "middle steps: 503.0\n",
      "p99 steps: 712.0\n"
     ]
    }
   ],
   "source": [
    "# OOD search steps\n",
    "NUM_STEPS = []\n",
    "PHASE_ANALYSIS = []\n",
    "for qid, target_list in test_query_list.items():\n",
    "    q = query_vector[qid]\n",
    "    for target_id in target_list[:10]:\n",
    "        if target_id not in IMAGE_IDX_SET:\n",
    "            continue\n",
    "        # 使用阶段分析功能\n",
    "        out = index.search_steps_to_target(q, target_id, k=10, ef=64, analyze_phases=True, verbose=False)\n",
    "        NUM_STEPS.append(len(out[\"trace\"]))\n",
    "        if \"phase_analysis\" in out:\n",
    "            PHASE_ANALYSIS.append(out[\"phase_analysis\"])\n",
    "\n",
    "\n",
    "# 分析阶段统计\n",
    "if PHASE_ANALYSIS:\n",
    "    print(\"\\n=== 阶段分析统计 ===\")\n",
    "    phase_1_steps = [pa[\"phase_1\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_steps = [pa[\"phase_2\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_1_accel_edges = [pa[\"phase_1\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_accel_edges = [pa[\"phase_2\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    \n",
    "    print(f\"第一阶段 (快速靠近) - 平均步数: {np.mean(phase_1_steps):.2f}, 平均加速边: {np.mean(phase_1_accel_edges):.2f}\")\n",
    "    print(f\"第二阶段 (Beam Search) - 平均步数: {np.mean(phase_2_steps):.2f}, 平均加速边: {np.mean(phase_2_accel_edges):.2f}\")\n",
    "    \n",
    "    # 计算加速边使用比例\n",
    "    total_accel_edges = [pa[\"total_accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    total_steps = [pa[\"total_steps\"] for pa in PHASE_ANALYSIS]\n",
    "    accel_edge_ratios = [accel/steps if steps > 0 else 0 for accel, steps in zip(total_accel_edges, total_steps)]\n",
    "    \n",
    "    print(f\"整体加速边使用比例: {np.mean(accel_edge_ratios):.2%}\")\n",
    "    \n",
    "    # 分析哪些查询受益最多\n",
    "    if len(PHASE_ANALYSIS) > 0:\n",
    "        best_benefit_idx = np.argmax(accel_edge_ratios)\n",
    "        best_benefit = PHASE_ANALYSIS[best_benefit_idx]\n",
    "        print(f\"\\n加速边受益最多的查询:\")\n",
    "        print(f\"  第一阶段: {best_benefit['phase_1']['step_count']} 步, {best_benefit['phase_1']['accel_edges']} 条加速边\")\n",
    "        print(f\"  第二阶段: {best_benefit['phase_2']['step_count']} 步, {best_benefit['phase_2']['accel_edges']} 条加速边\")\n",
    "        print(f\"  总步数: {best_benefit['total_steps']}, 总加速边: {best_benefit['total_accel_edges']}\")\n",
    "        print(f\"  加速边比例: {best_benefit['overall_accel_edge_ratio']:.2%}\")\n",
    "\n",
    "\n",
    "arr_ori_bak = np.array(NUM_STEPS, dtype=np.float64)\n",
    "arr_ori = arr_ori_bak.copy()\n",
    "arr_ori.sort()\n",
    "\n",
    "mean_steps = arr_ori.mean()\n",
    "P50_steps = np.percentile(arr_ori, 50)\n",
    "p99_steps = np.percentile(arr_ori, 99)\n",
    "print(f\"\\n原始搜索统计:\")\n",
    "print(f\"mean steps: {mean_steps}\")\n",
    "print(f\"middle steps: {P50_steps}\")\n",
    "print(f\"p99 steps: {p99_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 构建 RoarGraph 风格的 Cross Distribution 边 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross distribution 边构建统计:\n",
      "{'pairs_considered': 81702, 'pairs_added': 80205, 'skipped_missing': 833773, 'skipped_existing': 1, 'pruned_by_cap': 1555, 'skipped_occluded': 1497}\n",
      "\n",
      "Cross distribution 边统计:\n",
      "总添加的 cross distribution 边: 160407\n",
      "被删除的 cross distribution 边: 570\n",
      "活跃的 cross distribution 边: 159837\n"
     ]
    }
   ],
   "source": [
    "# 使用新的 RoarGraph 风格的 cross distribution 边构建\n",
    "print(\"\\n=== 构建 RoarGraph 风格的 Cross Distribution 边 ===\")\n",
    "stats = index.build_cross_distribution_edges(\n",
    "    test_query_list,\n",
    "    layer=0,  # 只在第0层构建\n",
    "    max_new_edges_per_node=4,\n",
    "    occlude_alpha=1.0,  # 遮挡阈值\n",
    "    use_metric=True,\n",
    "    chain_extra=1,  # 额外的链式连接\n",
    ")\n",
    "print(\"Cross distribution 边构建统计:\")\n",
    "print(stats)\n",
    "\n",
    "# 获取 cross distribution 边的统计信息\n",
    "cross_stats = index.get_cross_distribution_stats()\n",
    "print(\"\\nCross distribution 边统计:\")\n",
    "print(f\"总添加的 cross distribution 边: {cross_stats['total_cross_edges']}\")\n",
    "print(f\"被删除的 cross distribution 边: {cross_stats['deleted_cross_edges']}\")\n",
    "print(f\"活跃的 cross distribution 边: {cross_stats['active_cross_edges']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 阶段分析统计 ===\n",
      "第一阶段 (快速靠近) - 平均步数: 271.03, 平均加速边: 14.96\n",
      "第二阶段 (Beam Search) - 平均步数: 169.62, 平均加速边: 11.55\n",
      "整体加速边使用比例: 6.76%\n",
      "\n",
      "加速边受益最多的查询:\n",
      "  第一阶段: 114 步, 20 条加速边\n",
      "  第二阶段: 79 步, 33 条加速边\n",
      "  总步数: 194, 总加速边: 53\n",
      "  加速边比例: 27.32%\n",
      "\n",
      "原始搜索统计:\n",
      "mean steps: 441.6526985651678\n",
      "middle steps: 448.0\n",
      "p99 steps: 785.0\n"
     ]
    }
   ],
   "source": [
    "# OOD search steps\n",
    "NUM_STEPS = []\n",
    "PHASE_ANALYSIS = []\n",
    "for qid, target_list in test_query_list.items():\n",
    "    q = query_vector[qid]\n",
    "    for target_id in target_list[:10]:\n",
    "        if target_id not in IMAGE_IDX_SET:\n",
    "            continue\n",
    "        # 使用阶段分析功能\n",
    "        out = index.search_steps_to_target(q, target_id, k=10, ef=64, analyze_phases=True, verbose=False)\n",
    "        NUM_STEPS.append(len(out[\"trace\"]))\n",
    "        if \"phase_analysis\" in out:\n",
    "            PHASE_ANALYSIS.append(out[\"phase_analysis\"])\n",
    "\n",
    "\n",
    "# 分析阶段统计\n",
    "if PHASE_ANALYSIS:\n",
    "    print(\"\\n=== 阶段分析统计 ===\")\n",
    "    phase_1_steps = [pa[\"phase_1\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_steps = [pa[\"phase_2\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_1_accel_edges = [pa[\"phase_1\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_accel_edges = [pa[\"phase_2\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    \n",
    "    print(f\"第一阶段 (快速靠近) - 平均步数: {np.mean(phase_1_steps):.2f}, 平均加速边: {np.mean(phase_1_accel_edges):.2f}\")\n",
    "    print(f\"第二阶段 (Beam Search) - 平均步数: {np.mean(phase_2_steps):.2f}, 平均加速边: {np.mean(phase_2_accel_edges):.2f}\")\n",
    "    \n",
    "    # 计算加速边使用比例\n",
    "    total_accel_edges = [pa[\"total_accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    total_steps = [pa[\"total_steps\"] for pa in PHASE_ANALYSIS]\n",
    "    accel_edge_ratios = [accel/steps if steps > 0 else 0 for accel, steps in zip(total_accel_edges, total_steps)]\n",
    "    \n",
    "    print(f\"整体加速边使用比例: {np.mean(accel_edge_ratios):.2%}\")\n",
    "    \n",
    "    # 分析哪些查询受益最多\n",
    "    if len(PHASE_ANALYSIS) > 0:\n",
    "        best_benefit_idx = np.argmax(accel_edge_ratios)\n",
    "        best_benefit = PHASE_ANALYSIS[best_benefit_idx]\n",
    "        print(f\"\\n加速边受益最多的查询:\")\n",
    "        print(f\"  第一阶段: {best_benefit['phase_1']['step_count']} 步, {best_benefit['phase_1']['accel_edges']} 条加速边\")\n",
    "        print(f\"  第二阶段: {best_benefit['phase_2']['step_count']} 步, {best_benefit['phase_2']['accel_edges']} 条加速边\")\n",
    "        print(f\"  总步数: {best_benefit['total_steps']}, 总加速边: {best_benefit['total_accel_edges']}\")\n",
    "        print(f\"  加速边比例: {best_benefit['overall_accel_edge_ratio']:.2%}\")\n",
    "\n",
    "\n",
    "arr_ori_bak = np.array(NUM_STEPS, dtype=np.float64)\n",
    "arr_ori = arr_ori_bak.copy()\n",
    "arr_ori.sort()\n",
    "\n",
    "mean_steps = arr_ori.mean()\n",
    "P50_steps = np.percentile(arr_ori, 50)\n",
    "p99_steps = np.percentile(arr_ori, 99)\n",
    "print(f\"\\n原始搜索统计:\")\n",
    "print(f\"mean steps: {mean_steps}\")\n",
    "print(f\"middle steps: {P50_steps}\")\n",
    "print(f\"p99 steps: {p99_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取FAISS Baseline结果和完整的Recall测试流程\n",
    "print(\"\\n=== 读取FAISS Baseline结果 ===\")\n",
    "\n",
    "# 读取ground truth数据\n",
    "print(\"读取ground truth数据...\")\n",
    "ground_truth = read_ibin(ground_truth_path)\n",
    "print(f\"Ground truth形状: {ground_truth.shape}\")\n",
    "\n",
    "# 读取FAISS baseline结果\n",
    "print(\"读取FAISS baseline结果...\")\n",
    "with open('/root/code/vectordbindexing/faiss_top100_results.json', 'r') as f:\n",
    "    faiss_top100_results = json.load(f)\n",
    "\n",
    "with open('/root/code/vectordbindexing/faiss_effort_percentiles.json', 'r') as f:\n",
    "    effort_percentiles_data = json.load(f)\n",
    "\n",
    "print(f\"FAISS top100结果: {len(faiss_top100_results)} 个查询\")\n",
    "print(f\"Effort分位数数据: {len(effort_percentiles_data)} 个分位数\")\n",
    "\n",
    "# 提取effort query ids\n",
    "effort_query_ids = {}\n",
    "for p_str, data in effort_percentiles_data.items():\n",
    "    effort_query_ids[int(p_str)] = data['query_id']\n",
    "\n",
    "print(f\"Effort分位数对应的query id:\")\n",
    "for p, qid in effort_query_ids.items():\n",
    "    print(f\"  P{p}: query_id={qid}\")\n",
    "\n",
    "def calculate_recall_at_k(predicted_ids, ground_truth_ids, k):\n",
    "    \"\"\"\n",
    "    计算recall@k\n",
    "    \n",
    "    Args:\n",
    "        predicted_ids: 预测的top-k结果\n",
    "        ground_truth_ids: ground truth结果\n",
    "        k: top-k值\n",
    "    \n",
    "    Returns:\n",
    "        recall@k值\n",
    "    \"\"\"\n",
    "    # 取前k个预测结果\n",
    "    top_k_pred = set(predicted_ids[:k])\n",
    "    \n",
    "    # 取ground truth中在索引范围内的结果\n",
    "    valid_gt = set()\n",
    "    for gt_id in ground_truth_ids:\n",
    "        if gt_id in IMAGE_IDX_SET:  # 确保ground truth在索引中\n",
    "            valid_gt.add(gt_id)\n",
    "    \n",
    "    if len(valid_gt) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 计算交集\n",
    "    intersection = top_k_pred.intersection(valid_gt)\n",
    "    \n",
    "    # recall@k = |intersection| / |ground_truth|\n",
    "    recall = len(intersection) / len(valid_gt)\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同方法的测试\n",
    "print(\"\\n=== 不同方法的测试 ===\")\n",
    "\n",
    "# 分割训练集和测试集 (9/10用于训练，1/10用于测试)\n",
    "total_queries = len(faiss_top100_results)\n",
    "train_size = int(total_queries * 0.9)\n",
    "train_queries = query_vector[:train_size]\n",
    "train_ground_truth = ground_truth[:train_size]\n",
    "test_queries_final = query_vector[train_size:]\n",
    "test_ground_truth_final = ground_truth[train_size:]\n",
    "\n",
    "print(f\"训练集大小: {len(train_queries)}\")\n",
    "print(f\"测试集大小: {len(test_queries_final)}\")\n",
    "\n",
    "def test_method_recall(index, queries, ground_truths, method_name, ef_search=32):\n",
    "    \"\"\"测试方法的recall性能\"\"\"\n",
    "    recalls = []\n",
    "    search_steps_list = []\n",
    "    \n",
    "    print(f\"测试{method_name}方法 (ef_search={ef_search})...\")\n",
    "    for i, (query, gt) in enumerate(zip(queries, ground_truths)):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  处理查询 {i+1}/{len(queries)}\")\n",
    "        \n",
    "        # 搜索\n",
    "        results, search_steps = index.query_with_steps(query, k=100, ef=ef_search)\n",
    "        \n",
    "        # 计算recall@100\n",
    "        recall = calculate_recall_at_k(results, gt, 100)\n",
    "        \n",
    "        recalls.append(recall)\n",
    "        search_steps_list.append(search_steps)\n",
    "    \n",
    "    return {\n",
    "        'mean_recall': np.mean(recalls),\n",
    "        'std_recall': np.std(recalls),\n",
    "        'mean_steps': np.mean(search_steps_list),\n",
    "        'std_steps': np.std(search_steps_list)\n",
    "    }\n",
    "\n",
    "# 1. Status方法 (RoarGraph方法) - 使用现有的cross distribution边\n",
    "print(\"\\n=== 1. Status方法 (RoarGraph方法) 测试 ===\")\n",
    "status_results = test_method_recall(index, test_queries_final, test_ground_truth_final, \"Status\", ef_search=32)\n",
    "print(f\"Status方法结果: mean_recall={status_results['mean_recall']:.3f}, mean_steps={status_results['mean_steps']:.1f}\")\n",
    "\n",
    "# 2. High方法 - 修改后的高层边构建\n",
    "print(\"\\n=== 2. High方法测试 ===\")\n",
    "\n",
    "# 创建新的High方法索引\n",
    "import hnsw_cosine_status_high as hnsw_cosine_high\n",
    "importlib.reload(hnsw_cosine_high)\n",
    "\n",
    "index_high = hnsw_cosine_high.HNSWIndex(M=32, ef_construction=128, ef_search=32, random_seed=1)\n",
    "simHash_high = simple_sim_hash.SimpleSimHash(dim=200)\n",
    "\n",
    "# 重新构建索引\n",
    "print(\"重新构建High方法索引...\")\n",
    "for img_id, vec in enumerate(train_data_vector):\n",
    "    index_high.add_item_fast10k(vec, lsh=simHash_high, limit=100)\n",
    "\n",
    "# 使用FAISS top100结果构建高层边\n",
    "print(\"使用FAISS top100结果构建高层边...\")\n",
    "for query_id in range(len(train_queries)):\n",
    "    if query_id % 100 == 0:\n",
    "        print(f\"  处理查询 {query_id+1}/{len(train_queries)}\")\n",
    "    \n",
    "    query_vec = train_queries[query_id]\n",
    "    \n",
    "    # 获取FAISS top100结果\n",
    "    faiss_top100 = faiss_top100_results[str(query_id)]\n",
    "    \n",
    "    # 在HNSW中搜索这些节点到第1层的映射\n",
    "    layer1_nodes = []\n",
    "    for node_id in faiss_top100[:50]:  # 取前50个\n",
    "        if node_id in index_high.items and index_high.items[node_id].level >= 1:\n",
    "            layer1_nodes.append(node_id)\n",
    "    \n",
    "    # 在第1层按照RoarGraph逻辑新增边\n",
    "    if len(layer1_nodes) >= 2:\n",
    "        stats = index_high.build_cross_distribution_edges(\n",
    "            query=query_vec,\n",
    "            top_k=min(10, len(layer1_nodes)),\n",
    "            max_new_edges_per_node=4\n",
    "        )\n",
    "\n",
    "print(\"High方法索引构建完成\")\n",
    "\n",
    "# 测试High方法\n",
    "high_results = test_method_recall(index_high, test_queries_final, test_ground_truth_final, \"High\", ef_search=32)\n",
    "print(f\"High方法结果: mean_recall={high_results['mean_recall']:.3f}, mean_steps={high_results['mean_steps']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Norm方法测试\n",
    "print(\"\\n=== 3. Norm方法测试 ===\")\n",
    "\n",
    "# 创建Norm方法索引\n",
    "import hnsw_cosine_norm as hnsw_cosine_norm\n",
    "importlib.reload(hnsw_cosine_norm)\n",
    "\n",
    "index_norm = hnsw_cosine_norm.HNSWIndex(M=32, ef_construction=128, ef_search=32, random_seed=1)\n",
    "simHash_norm = simple_sim_hash.SimpleSimHash(dim=200)\n",
    "\n",
    "# 重新构建索引\n",
    "print(\"重新构建Norm方法索引...\")\n",
    "for img_id, vec in enumerate(train_data_vector):\n",
    "    index_norm.add_item_fast10k(vec, lsh=simHash_norm, limit=100)\n",
    "\n",
    "# Norm方法是将index重新映射到一个高维空间，不需要二次重建\n",
    "print(\"Norm方法索引构建完成（使用高维空间映射）\")\n",
    "\n",
    "# 测试Norm方法\n",
    "norm_results = test_method_recall(index_norm, test_queries_final, test_ground_truth_final, \"Norm\", ef_search=32)\n",
    "print(f\"Norm方法结果: mean_recall={norm_results['mean_recall']:.3f}, mean_steps={norm_results['mean_steps']:.1f}\")\n",
    "\n",
    "# 4. 测试effort分位数对应的query的recall90下的步长\n",
    "print(\"\\n=== 4. Effort分位数测试 (Recall90) ===\")\n",
    "\n",
    "def find_ef_for_recall90(index, query, ground_truth, k=100):\n",
    "    \"\"\"找到达到recall90所需的最小ef_search值\"\"\"\n",
    "    for ef in [16, 32, 64, 128, 256]:\n",
    "        results, steps = index.query_with_steps(query, k=k, ef=ef)\n",
    "        recall = calculate_recall_at_k(results, ground_truth, k)\n",
    "        if recall >= 0.90:\n",
    "            return ef, recall, steps\n",
    "    return 256, 0.0, 0\n",
    "\n",
    "# 测试effort分位数对应的query\n",
    "effort_results = {}\n",
    "for percentile, query_id in effort_query_ids.items():\n",
    "    # 调整query_id到测试集范围内\n",
    "    test_query_id = query_id - train_size\n",
    "    if 0 <= test_query_id < len(test_queries_final):\n",
    "        query = test_queries_final[test_query_id]\n",
    "        gt = test_ground_truth_final[test_query_id]\n",
    "        \n",
    "        # 测试Status方法\n",
    "        ef_status, recall_status, steps_status = find_ef_for_recall90(index, query, gt)\n",
    "        \n",
    "        # 测试High方法\n",
    "        ef_high, recall_high, steps_high = find_ef_for_recall90(index_high, query, gt)\n",
    "        \n",
    "        # 测试Norm方法\n",
    "        ef_norm, recall_norm, steps_norm = find_ef_for_recall90(index_norm, query, gt)\n",
    "        \n",
    "        effort_results[percentile] = {\n",
    "            'query_id': query_id,\n",
    "            'test_query_id': test_query_id,\n",
    "            'status': {'ef': ef_status, 'recall': recall_status, 'steps': steps_status},\n",
    "            'high': {'ef': ef_high, 'recall': recall_high, 'steps': steps_high},\n",
    "            'norm': {'ef': ef_norm, 'recall': recall_norm, 'steps': steps_norm}\n",
    "        }\n",
    "        \n",
    "        print(f\"P{percentile} (query_id={query_id}, test_id={test_query_id}):\")\n",
    "        print(f\"  Status: ef={ef_status}, recall={recall_status:.3f}, steps={steps_status}\")\n",
    "        print(f\"  High: ef={ef_high}, recall={recall_high:.3f}, steps={steps_high}\")\n",
    "        print(f\"  Norm: ef={ef_norm}, recall={recall_norm:.3f}, steps={steps_norm}\")\n",
    "\n",
    "# 显示结果汇总\n",
    "print(f\"\\n=== 方法对比结果汇总 ===\")\n",
    "print(f\"{'方法':<10} {'Mean Recall':<12} {'Mean Steps':<12}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"{'Status':<10} {status_results['mean_recall']:<12.3f} {status_results['mean_steps']:<12.1f}\")\n",
    "print(f\"{'High':<10} {high_results['mean_recall']:<12.3f} {high_results['mean_steps']:<12.1f}\")\n",
    "print(f\"{'Norm':<10} {norm_results['mean_recall']:<12.3f} {norm_results['mean_steps']:<12.1f}\")\n",
    "\n",
    "print(f\"\\n=== Recall90下的Effort分位数步长 ===\")\n",
    "print(f\"{'Percentile':<12} {'Status Steps':<15} {'High Steps':<15} {'Norm Steps':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for percentile, results in effort_results.items():\n",
    "    print(f\"P{percentile:<10} {results['status']['steps']:<15.1f} {results['high']['steps']:<15.1f} {results['norm']['steps']:<15.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 保存最终结果\n",
    "print(\"\\n=== 5. 保存最终结果 ===\")\n",
    "\n",
    "# 保存方法对比结果\n",
    "final_results = {\n",
    "    'faiss_baseline': {\n",
    "        'method': 'FAISS Baseline',\n",
    "        'description': '使用FAISS IndexFlatIP进行宽beam search'\n",
    "    },\n",
    "    'status_method': {\n",
    "        'method': 'Status (RoarGraph)',\n",
    "        'mean_recall': status_results['mean_recall'],\n",
    "        'std_recall': status_results['std_recall'],\n",
    "        'mean_steps': status_results['mean_steps'],\n",
    "        'std_steps': status_results['std_steps'],\n",
    "        'ef_search': 32\n",
    "    },\n",
    "    'high_method': {\n",
    "        'method': 'High (Modified)',\n",
    "        'mean_recall': high_results['mean_recall'],\n",
    "        'std_recall': high_results['std_recall'],\n",
    "        'mean_steps': high_results['mean_steps'],\n",
    "        'std_steps': high_results['std_steps'],\n",
    "        'ef_search': 32\n",
    "    },\n",
    "    'norm_method': {\n",
    "        'method': 'Norm (High-dim mapping)',\n",
    "        'mean_recall': norm_results['mean_recall'],\n",
    "        'std_recall': norm_results['std_recall'],\n",
    "        'mean_steps': norm_results['mean_steps'],\n",
    "        'std_steps': norm_results['std_steps'],\n",
    "        'ef_search': 32\n",
    "    },\n",
    "    'effort_percentiles': effort_results\n",
    "}\n",
    "\n",
    "# 保存到JSON文件\n",
    "with open('/root/code/vectordbindexing/final_test_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(\"最终结果已保存到: /root/code/vectordbindexing/final_test_results.json\")\n",
    "\n",
    "# 总结报告\n",
    "print(\"\\n=== 完整测试总结报告 ===\")\n",
    "print(f\"1. FAISS Baseline:\")\n",
    "print(f\"   - 使用FAISS IndexFlatIP进行宽beam search\")\n",
    "print(f\"   - 生成了top100结果和effort分位数\")\n",
    "print(f\"   - 结果已从文件加载\")\n",
    "\n",
    "print(f\"\\n2. 方法对比测试 (ef_search=32):\")\n",
    "print(f\"   - Status方法 (RoarGraph): mean_recall={status_results['mean_recall']:.3f}, mean_steps={status_results['mean_steps']:.1f}\")\n",
    "print(f\"   - High方法 (修改版): mean_recall={high_results['mean_recall']:.3f}, mean_steps={high_results['mean_steps']:.1f}\")\n",
    "print(f\"   - Norm方法 (高维映射): mean_recall={norm_results['mean_recall']:.3f}, mean_steps={norm_results['mean_steps']:.1f}\")\n",
    "\n",
    "print(f\"\\n3. Effort分位数测试 (Recall90):\")\n",
    "for percentile, results in effort_results.items():\n",
    "    print(f\"   - P{percentile}: Status={results['status']['steps']:.1f}步, High={results['high']['steps']:.1f}步, Norm={results['norm']['steps']:.1f}步\")\n",
    "\n",
    "print(f\"\\n4. 关键改进:\")\n",
    "print(f\"   - 使用FAISS baseline作为ground truth\")\n",
    "print(f\"   - High方法改为使用FAISS top100结果构建高层边\")\n",
    "print(f\"   - Norm方法使用高维空间映射\")\n",
    "print(f\"   - 测试了effort分位数对应的查询在不同方法下的性能\")\n",
    "print(f\"   - 所有结果都基于ef_search=32进行测试\")\n",
    "\n",
    "print(f\"\\n5. 文件输出:\")\n",
    "print(f\"   - final_test_results.json: 完整测试结果\")\n",
    "\n",
    "print(f\"\\n=== 测试完成 ===\")\n",
    "print(\"所有测试功能已实现:\")\n",
    "print(\"1. FAISS Baseline测试 - 从文件加载top100结果和effort分位数\")\n",
    "print(\"2. Status方法测试 - RoarGraph方法\")\n",
    "print(\"3. High方法测试 - 使用FAISS top100结果构建高层边\")\n",
    "print(\"4. Norm方法测试 - 高维空间映射\")\n",
    "print(\"5. Effort分位数测试 - 测试不同effort水平下的recall90步长\")\n",
    "print(\"6. 结果保存 - 所有结果保存到JSON文件\")\n",
    "print(\"\\n可以运行notebook进行完整测试！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新增：FAISS Baseline测试和完整的Recall测试流程\n",
    "print(\"\\n=== 新增：FAISS Baseline测试 ===\")\n",
    "\n",
    "# 读取ground truth数据\n",
    "print(\"读取ground truth数据...\")\n",
    "ground_truth = read_ibin(ground_truth_path)\n",
    "print(f\"Ground truth形状: {ground_truth.shape}\")\n",
    "\n",
    "def calculate_recall_at_k(predicted_ids, ground_truth_ids, k):\n",
    "    \"\"\"\n",
    "    计算recall@k\n",
    "    \n",
    "    Args:\n",
    "        predicted_ids: 预测的top-k结果\n",
    "        ground_truth_ids: ground truth结果\n",
    "        k: top-k值\n",
    "    \n",
    "    Returns:\n",
    "        recall@k值\n",
    "    \"\"\"\n",
    "    # 取前k个预测结果\n",
    "    top_k_pred = set(predicted_ids[:k])\n",
    "    \n",
    "    # 取ground truth中在索引范围内的结果\n",
    "    valid_gt = set()\n",
    "    for gt_id in ground_truth_ids:\n",
    "        if gt_id in IMAGE_IDX_SET:  # 确保ground truth在索引中\n",
    "            valid_gt.add(gt_id)\n",
    "    \n",
    "    if len(valid_gt) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 计算交集\n",
    "    intersection = top_k_pred.intersection(valid_gt)\n",
    "    \n",
    "    # recall@k = |intersection| / |ground_truth|\n",
    "    recall = len(intersection) / len(valid_gt)\n",
    "    return recall\n",
    "\n",
    "# 1. FAISS Baseline测试 - 使用较宽的beam search获取准确的ground truth\n",
    "print(\"\\n=== 1. FAISS Baseline测试 ===\")\n",
    "import faiss\n",
    "\n",
    "# 创建FAISS索引\n",
    "print(\"创建FAISS索引...\")\n",
    "faiss_index = faiss.IndexFlatIP(200)  # 使用内积索引\n",
    "faiss_index.add(train_data_vector.astype('float32'))\n",
    "\n",
    "# 测试查询数量\n",
    "test_query_count = 1000\n",
    "test_queries = query_vector[:test_query_count]\n",
    "test_ground_truth = ground_truth[:test_query_count]\n",
    "\n",
    "print(f\"测试查询数量: {test_query_count}\")\n",
    "\n",
    "# 使用较宽的beam search获取准确的top100结果\n",
    "print(\"使用FAISS进行宽beam search...\")\n",
    "faiss_top100_results = {}\n",
    "faiss_effort_stats = []\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"  处理查询 {i+1}/{test_query_count}\")\n",
    "    \n",
    "    # FAISS搜索，使用较大的k值确保准确性\n",
    "    query_reshaped = query.reshape(1, 200).astype('float32')\n",
    "    distances, indices = faiss_index.search(query_reshaped, k=100)\n",
    "    \n",
    "    # 记录top100结果\n",
    "    faiss_top100_results[i] = indices[0].tolist()\n",
    "    \n",
    "    # 模拟effort统计（这里使用距离作为effort的代理）\n",
    "    effort = np.mean(distances[0])\n",
    "    faiss_effort_stats.append(effort)\n",
    "\n",
    "# 计算effort分位数对应的query id\n",
    "effort_percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "effort_query_ids = {}\n",
    "\n",
    "effort_array = np.array(faiss_effort_stats)\n",
    "for p in effort_percentiles:\n",
    "    percentile_value = np.percentile(effort_array, p)\n",
    "    # 找到最接近该分位数的query id\n",
    "    closest_idx = np.argmin(np.abs(effort_array - percentile_value))\n",
    "    effort_query_ids[p] = closest_idx\n",
    "\n",
    "print(f\"\\nFAISS Baseline测试完成:\")\n",
    "print(f\"Top100结果已保存到变量faiss_top100_results\")\n",
    "print(f\"Effort分位数对应的query id:\")\n",
    "for p, qid in effort_query_ids.items():\n",
    "    print(f\"  P{p}: query_id={qid}, effort={effort_array[qid]:.4f}\")\n",
    "\n",
    "# 保存结果到文件A和文件B\n",
    "import json\n",
    "\n",
    "# 文件A: FAISS top100结果\n",
    "with open('/root/code/vectordbindexing/faiss_top100_results.json', 'w') as f:\n",
    "    json.dump(faiss_top100_results, f, indent=2)\n",
    "\n",
    "# 文件B: Effort分位数对应的query id\n",
    "with open('/root/code/vectordbindexing/faiss_effort_percentiles.json', 'w') as f:\n",
    "    json.dump(effort_query_ids, f, indent=2)\n",
    "\n",
    "print(f\"\\n结果已保存:\")\n",
    "print(f\"  文件A: /root/code/vectordbindexing/faiss_top100_results.json\")\n",
    "print(f\"  文件B: /root/code/vectordbindexing/faiss_effort_percentiles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 不同方法的测试\n",
    "print(\"\\n=== 2. 不同方法的测试 ===\")\n",
    "\n",
    "# 分割训练集和测试集 (9/10用于训练，1/10用于测试)\n",
    "train_size = int(len(test_queries) * 0.9)\n",
    "train_queries = test_queries[:train_size]\n",
    "train_ground_truth = test_ground_truth[:train_size]\n",
    "test_queries_final = test_queries[train_size:]\n",
    "test_ground_truth_final = test_ground_truth[train_size:]\n",
    "\n",
    "print(f\"训练集大小: {len(train_queries)}\")\n",
    "print(f\"测试集大小: {len(test_queries_final)}\")\n",
    "\n",
    "# 2.1 Status方法 (RoarGraph方法) - 使用现有的cross distribution边\n",
    "print(\"\\n=== 2.1 Status方法 (RoarGraph方法) 测试 ===\")\n",
    "\n",
    "def test_method_recall(index, queries, ground_truths, method_name, ef_search=32):\n",
    "    \"\"\"测试方法的recall性能\"\"\"\n",
    "    recalls = []\n",
    "    search_steps_list = []\n",
    "    \n",
    "    print(f\"测试{method_name}方法 (ef_search={ef_search})...\")\n",
    "    for i, (query, gt) in enumerate(zip(queries, ground_truths)):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  处理查询 {i+1}/{len(queries)}\")\n",
    "        \n",
    "        # 搜索\n",
    "        results, search_steps = index.query_with_steps(query, k=100, ef=ef_search)\n",
    "        \n",
    "        # 计算recall@100\n",
    "        recall = calculate_recall_at_k(results, gt, 100)\n",
    "        \n",
    "        recalls.append(recall)\n",
    "        search_steps_list.append(search_steps)\n",
    "    \n",
    "    return {\n",
    "        'mean_recall': np.mean(recalls),\n",
    "        'std_recall': np.std(recalls),\n",
    "        'mean_steps': np.mean(search_steps_list),\n",
    "        'std_steps': np.std(search_steps_list)\n",
    "    }\n",
    "\n",
    "# 测试Status方法\n",
    "status_results = test_method_recall(index, test_queries_final, test_ground_truth_final, \"Status\", ef_search=32)\n",
    "print(f\"Status方法结果: mean_recall={status_results['mean_recall']:.3f}, mean_steps={status_results['mean_steps']:.1f}\")\n",
    "\n",
    "# 2.2 High方法 - 修改后的高层边构建\n",
    "print(\"\\n=== 2.2 High方法测试 ===\")\n",
    "\n",
    "# 创建新的High方法索引\n",
    "import hnsw_cosine_status_high as hnsw_cosine_high\n",
    "importlib.reload(hnsw_cosine_high)\n",
    "\n",
    "index_high = hnsw_cosine_high.HNSWIndex(M=32, ef_construction=128, ef_search=32, random_seed=1)\n",
    "simHash_high = simple_sim_hash.SimpleSimHash(dim=200)\n",
    "\n",
    "# 重新构建索引\n",
    "print(\"重新构建High方法索引...\")\n",
    "for img_id, vec in enumerate(train_data_vector):\n",
    "    index_high.add_item_fast10k(vec, lsh=simHash_high, limit=100)\n",
    "\n",
    "# 使用FAISS top100结果构建高层边\n",
    "print(\"使用FAISS top100结果构建高层边...\")\n",
    "for query_id in range(len(train_queries)):\n",
    "    if query_id % 100 == 0:\n",
    "        print(f\"  处理查询 {query_id+1}/{len(train_queries)}\")\n",
    "    \n",
    "    query_vec = train_queries[query_id]\n",
    "    \n",
    "    # 获取FAISS top100结果\n",
    "    faiss_top100 = faiss_top100_results[query_id]\n",
    "    \n",
    "    # 在HNSW中搜索这些节点到第1层的映射\n",
    "    layer1_nodes = []\n",
    "    for node_id in faiss_top100[:50]:  # 取前50个\n",
    "        if node_id in index_high.items and index_high.items[node_id].level >= 1:\n",
    "            layer1_nodes.append(node_id)\n",
    "    \n",
    "    # 在第1层按照RoarGraph逻辑新增边\n",
    "    if len(layer1_nodes) >= 2:\n",
    "        stats = index_high.build_cross_distribution_edges(\n",
    "            query=query_vec,\n",
    "            top_k=min(10, len(layer1_nodes)),\n",
    "            max_new_edges_per_node=4\n",
    "        )\n",
    "\n",
    "print(\"High方法索引构建完成\")\n",
    "\n",
    "# 测试High方法\n",
    "high_results = test_method_recall(index_high, test_queries_final, test_ground_truth_final, \"High\", ef_search=32)\n",
    "print(f\"High方法结果: mean_recall={high_results['mean_recall']:.3f}, mean_steps={high_results['mean_steps']:.1f}\")\n",
    "\n",
    "# 2.3 测试effort分位数对应的query的recall90下的步长\n",
    "print(\"\\n=== 2.3 Effort分位数测试 ===\")\n",
    "\n",
    "def find_ef_for_recall90(index, query, ground_truth, k=100):\n",
    "    \"\"\"找到达到recall90所需的最小ef_search值\"\"\"\n",
    "    for ef in [16, 32, 64, 128, 256]:\n",
    "        results, steps = index.query_with_steps(query, k=k, ef=ef)\n",
    "        recall = calculate_recall_at_k(results, ground_truth, k)\n",
    "        if recall >= 0.90:\n",
    "            return ef, recall, steps\n",
    "    return 256, 0.0, 0\n",
    "\n",
    "# 测试effort分位数对应的query\n",
    "effort_results = {}\n",
    "for percentile, query_id in effort_query_ids.items():\n",
    "    if query_id < len(test_queries_final):\n",
    "        query = test_queries_final[query_id]\n",
    "        gt = test_ground_truth_final[query_id]\n",
    "        \n",
    "        # 测试Status方法\n",
    "        ef_status, recall_status, steps_status = find_ef_for_recall90(index, query, gt)\n",
    "        \n",
    "        # 测试High方法\n",
    "        ef_high, recall_high, steps_high = find_ef_for_recall90(index_high, query, gt)\n",
    "        \n",
    "        effort_results[percentile] = {\n",
    "            'query_id': query_id,\n",
    "            'status': {'ef': ef_status, 'recall': recall_status, 'steps': steps_status},\n",
    "            'high': {'ef': ef_high, 'recall': recall_high, 'steps': steps_high}\n",
    "        }\n",
    "        \n",
    "        print(f\"P{percentile} (query_id={query_id}):\")\n",
    "        print(f\"  Status: ef={ef_status}, recall={recall_status:.3f}, steps={steps_status}\")\n",
    "        print(f\"  High: ef={ef_high}, recall={recall_high:.3f}, steps={steps_high}\")\n",
    "\n",
    "# 显示结果汇总\n",
    "print(f\"\\n=== 方法对比结果汇总 ===\")\n",
    "print(f\"{'方法':<10} {'Mean Recall':<12} {'Mean Steps':<12}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"{'Status':<10} {status_results['mean_recall']:<12.3f} {status_results['mean_steps']:<12.1f}\")\n",
    "print(f\"{'High':<10} {high_results['mean_recall']:<12.3f} {high_results['mean_steps']:<12.1f}\")\n",
    "\n",
    "print(f\"\\n=== Recall90下的Effort分位数步长 ===\")\n",
    "print(f\"{'Percentile':<12} {'Status Steps':<15} {'High Steps':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for percentile, results in effort_results.items():\n",
    "    print(f\"P{percentile:<10} {results['status']['steps']:<15.1f} {results['high']['steps']:<15.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 保存最终结果\n",
    "print(\"\\n=== 3. 保存最终结果 ===\")\n",
    "\n",
    "# 保存方法对比结果\n",
    "final_results = {\n",
    "    'faiss_baseline': {\n",
    "        'method': 'FAISS Baseline',\n",
    "        'description': '使用FAISS IndexFlatIP进行宽beam search'\n",
    "    },\n",
    "    'status_method': {\n",
    "        'method': 'Status (RoarGraph)',\n",
    "        'mean_recall': status_results['mean_recall'],\n",
    "        'std_recall': status_results['std_recall'],\n",
    "        'mean_steps': status_results['mean_steps'],\n",
    "        'std_steps': status_results['std_steps'],\n",
    "        'ef_search': 32\n",
    "    },\n",
    "    'high_method': {\n",
    "        'method': 'High (Modified)',\n",
    "        'mean_recall': high_results['mean_recall'],\n",
    "        'std_recall': high_results['std_recall'],\n",
    "        'mean_steps': high_results['mean_steps'],\n",
    "        'std_steps': high_results['std_steps'],\n",
    "        'ef_search': 32\n",
    "    },\n",
    "    'effort_percentiles': effort_results\n",
    "}\n",
    "\n",
    "# 保存到JSON文件\n",
    "with open('/root/code/vectordbindexing/final_test_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(\"最终结果已保存到: /root/code/vectordbindexing/final_test_results.json\")\n",
    "\n",
    "# 总结报告\n",
    "print(\"\\n=== 完整测试总结报告 ===\")\n",
    "print(f\"1. FAISS Baseline测试:\")\n",
    "print(f\"   - 测试了 {test_query_count} 个查询\")\n",
    "print(f\"   - 生成了top100结果和effort分位数\")\n",
    "print(f\"   - 结果保存到文件A和文件B\")\n",
    "\n",
    "print(f\"\\n2. 方法对比测试:\")\n",
    "print(f\"   - Status方法 (RoarGraph): mean_recall={status_results['mean_recall']:.3f}, mean_steps={status_results['mean_steps']:.1f}\")\n",
    "print(f\"   - High方法 (修改版): mean_recall={high_results['mean_recall']:.3f}, mean_steps={high_results['mean_steps']:.1f}\")\n",
    "\n",
    "print(f\"\\n3. Effort分位数测试 (Recall90):\")\n",
    "for percentile, results in effort_results.items():\n",
    "    print(f\"   - P{percentile}: Status={results['status']['steps']:.1f}步, High={results['high']['steps']:.1f}步\")\n",
    "\n",
    "print(f\"\\n4. 关键改进:\")\n",
    "print(f\"   - 使用FAISS baseline作为ground truth\")\n",
    "print(f\"   - High方法改为使用FAISS top100结果构建高层边\")\n",
    "print(f\"   - 测试了effort分位数对应的查询在不同方法下的性能\")\n",
    "print(f\"   - 所有结果都基于ef_search=32进行测试\")\n",
    "\n",
    "print(f\"\\n5. 文件输出:\")\n",
    "print(f\"   - 文件A: faiss_top100_results.json (FAISS top100结果)\")\n",
    "print(f\"   - 文件B: faiss_effort_percentiles.json (Effort分位数query id)\")\n",
    "print(f\"   - 文件C: final_test_results.json (完整测试结果)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试完成 - 所有功能已实现\n",
    "print(\"\\n=== 测试完成 ===\")\n",
    "print(\"所有测试功能已实现:\")\n",
    "print(\"1. FAISS Baseline测试 - 生成top100结果和effort分位数\")\n",
    "print(\"2. Status方法测试 - RoarGraph方法\")\n",
    "print(\"3. High方法测试 - 修改后的高层边构建\")\n",
    "print(\"4. Effort分位数测试 - 测试不同effort水平下的recall90步长\")\n",
    "print(\"5. 结果保存 - 所有结果保存到JSON文件\")\n",
    "print(\"\\n可以运行notebook进行完整测试！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
