{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== HNSW方法对比测试框架 ===\n",
            "三种方法:\n",
            "1. Status (RoarGraph方法)\n",
            "2. High (高层新增边方法)\n",
            "3. Norm (高维映射方法)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<module 'hnsw_cosine_norm' from '/root/code/vectordbindexing/hnsw_cosine_norm.py'>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# HNSW方法对比测试框架\n",
        "# 基于FAISS baseline的三种HNSW方法对比\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from io_utils import read_fbin, read_ibin\n",
        "import hnsw_cosine_status as hnsw_status\n",
        "import hnsw_cosine_status_high as hnsw_high\n",
        "import hnsw_cosine_norm as hnsw_norm\n",
        "import simple_sim_hash\n",
        "import importlib\n",
        "\n",
        "print(\"=== HNSW方法对比测试框架 ===\")\n",
        "print(\"三种方法:\")\n",
        "print(\"1. Status (RoarGraph方法)\")\n",
        "print(\"2. High (高层新增边方法)\")\n",
        "print(\"3. Norm (高维映射方法)\")\n",
        "\n",
        "# 重新加载模块\n",
        "importlib.reload(hnsw_status)\n",
        "importlib.reload(hnsw_high)\n",
        "importlib.reload(hnsw_norm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 1. 加载数据和FAISS baseline结果 ===\n",
            "读取数据...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "数据向量: (1000000, 200)\n",
            "查询向量: (100000, 200)\n",
            "训练数据: (500000, 200)\n",
            "加载FAISS baseline结果...\n",
            "✅ 成功加载FAISS结果:\n",
            "   - Top100结果: 100000 个查询\n",
            "   - Effort分位数: 7 个分位数\n"
          ]
        }
      ],
      "source": [
        "# 1. 加载数据和FAISS baseline结果\n",
        "print(\"\\n=== 1. 加载数据和FAISS baseline结果 ===\")\n",
        "\n",
        "# 数据路径\n",
        "file_path = \"/root/code/vectordbindexing/Text2Image/base.1M.fbin\"\n",
        "query_path = \"/root/code/vectordbindexing/Text2Image/query.public.100K.fbin\"\n",
        "ground_truth_path = \"/root/code/vectordbindexing/Text2Image/groundtruth.public.100K.ibin\"\n",
        "\n",
        "# 读取数据\n",
        "print(\"读取数据...\")\n",
        "data_vector = read_fbin(file_path)\n",
        "query_vector = read_fbin(query_path)\n",
        "print(f\"数据向量: {data_vector.shape}\")\n",
        "print(f\"查询向量: {query_vector.shape}\")\n",
        "\n",
        "# 使用前500K数据进行测试\n",
        "train_data_vector = data_vector[:500000]\n",
        "print(f\"训练数据: {train_data_vector.shape}\")\n",
        "\n",
        "# 加载FAISS baseline结果\n",
        "print(\"加载FAISS baseline结果...\")\n",
        "try:\n",
        "    with open('/root/code/vectordbindexing/faiss_top100_results.json', 'r') as f:\n",
        "        faiss_top100_results = json.load(f)\n",
        "    with open('/root/code/vectordbindexing/faiss_effort_percentiles.json', 'r') as f:\n",
        "        effort_percentiles = json.load(f)\n",
        "    print(f\"✅ 成功加载FAISS结果:\")\n",
        "    print(f\"   - Top100结果: {len(faiss_top100_results)} 个查询\")\n",
        "    print(f\"   - Effort分位数: {len(effort_percentiles)} 个分位数\")\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ 未找到FAISS baseline结果文件\")\n",
        "    print(\"请先运行 hnsw_baseline_analysis.ipynb 生成baseline结果\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ground truth形状: (100000, 100)\n",
            "Ground truth形状: torch.Size([0])\n",
            "Ground truth[0]: tensor([])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "ground_truth = read_ibin(ground_truth_path)\n",
        "print(f\"Ground truth形状: {ground_truth.shape}\")\n",
        "\n",
        "def clean_ground_truth(ground_truth_vector):\n",
        "    new_ground_truth = []\n",
        "    for j in ground_truth_vector:\n",
        "        if j >= len(train_data_vector):\n",
        "            continue\n",
        "        new_ground_truth.append(j)\n",
        "    return torch.tensor(new_ground_truth)\n",
        "\n",
        "ground_truth_0 = clean_ground_truth(ground_truth[0])\n",
        "print(f\"Ground truth形状: {ground_truth_0.shape}\")\n",
        "print(f\"Ground truth[0]: {ground_truth_0}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 2. 数据分割和Ground Truth准备 ===\n",
            "训练查询数量: 9000\n",
            "测试查询数量: 1000\n",
            "准备Ground Truth...\n",
            "测试集Ground Truth: 1000 个, test_ground_truth[0]: [450938, 433164, 273168, 197768, 446539, 114597, 166119, 351005, 297013, 387034, 118529, 270950, 443415, 475354, 119404, 481549, 348835, 351111, 380869, 354360, 46775, 104998, 57946, 222629, 46956, 26165, 327783, 142800, 498687, 454881, 417378, 15560, 396110, 241241, 392323, 365809, 439516, 141796, 482766, 74151, 478561, 161810, 115050, 30807, 65268, 26159, 76581, 313282, 99660, 496140, 91828, 270484, 307967, 339310, 72767, 76276, 35872, 351813, 342630, 6940, 187681, 218782, 453213, 316839, 498876, 305505, 443486, 77713, 355767, 423349, 20107, 387996, 158089, 58428, 96610, 133870, 11408, 364752, 142080, 328994, 366763, 91908, 431275, 241251, 309460, 465622, 71743, 314026, 495976, 312480, 497829, 474173, 4450, 153810, 133791, 441544, 92647, 86201, 322329, 162520]\n",
            "训练集Ground Truth: 9000 个\n",
            "测试集Ground Truth: 1000 个\n"
          ]
        }
      ],
      "source": [
        "# 2. 数据分割和Ground Truth准备\n",
        "print(\"\\n=== 2. 数据分割和Ground Truth准备 ===\")\n",
        "\n",
        "# 使用前10K个查询进行测试\n",
        "test_query_count = 10000\n",
        "test_queries = query_vector[:test_query_count]\n",
        "\n",
        "# 分割训练集和测试集 (9/10用于训练，1/10用于测试)\n",
        "train_size = int(test_query_count * 0.9)\n",
        "train_queries = test_queries[:train_size]\n",
        "test_queries_final = test_queries[train_size:]\n",
        "\n",
        "print(f\"训练查询数量: {len(train_queries)}\")\n",
        "print(f\"测试查询数量: {len(test_queries_final)}\")\n",
        "\n",
        "# 准备Ground Truth\n",
        "print(\"准备Ground Truth...\")\n",
        "train_ground_truth = {}\n",
        "test_ground_truth = {}\n",
        "\n",
        "for i in range(len(train_queries)):\n",
        "    if str(i) in faiss_top100_results:\n",
        "        train_ground_truth[i] = faiss_top100_results[str(i)]\n",
        "\n",
        "for i in range(len(test_queries_final)):\n",
        "    global_idx = train_size + i\n",
        "    if str(global_idx) in faiss_top100_results:\n",
        "        test_ground_truth[i] = faiss_top100_results[str(global_idx)]\n",
        "print(f\"测试集Ground Truth: {len(test_ground_truth)} 个, test_ground_truth[0]: {test_ground_truth[0]}\")\n",
        "\n",
        "print(f\"训练集Ground Truth: {len(train_ground_truth)} 个\")\n",
        "print(f\"测试集Ground Truth: {len(test_ground_truth)} 个\")\n",
        "\n",
        "def calculate_recall_at_k(predicted_ids, ground_truth_ids, k):\n",
        "    \"\"\"计算recall@k\"\"\"\n",
        "    if len(ground_truth_ids) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    top_k_pred = set(predicted_ids[:k])\n",
        "    valid_gt = set(ground_truth_ids)\n",
        "    \n",
        "    intersection = top_k_pred.intersection(valid_gt)\n",
        "    recall = len(intersection) / len(valid_gt)\n",
        "    return recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 3. 创建三种HNSW索引 ===\n",
            "\n",
            "3.1 创建Status方法索引...\n",
            "添加训练数据到Status索引...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status索引构建完成，耗时: 1230.02秒\n",
            "构建cross distribution边...\n",
            "  处理查询 1/1000\n",
            "  处理查询 101/1000\n",
            "  处理查询 201/1000\n",
            "  处理查询 301/1000\n",
            "  处理查询 401/1000\n",
            "  处理查询 501/1000\n",
            "  处理查询 601/1000\n",
            "  处理查询 701/1000\n",
            "  处理查询 801/1000\n",
            "  处理查询 901/1000\n",
            "Status cross distribution边构建完成，耗时: 0.20秒\n"
          ]
        }
      ],
      "source": [
        "# 3. 创建三种HNSW索引\n",
        "print(\"\\n=== 3. 创建三种HNSW索引 ===\")\n",
        "\n",
        "# 通用参数\n",
        "M = 64\n",
        "ef_construction = 128\n",
        "ef_search = 200\n",
        "\n",
        "# 3.1 Status方法 (RoarGraph方法)\n",
        "print(\"\\n3.1 创建Status方法索引...\")\n",
        "index_status = hnsw_status.HNSWIndex(M=M, ef_construction=ef_construction, ef_search=ef_search, random_seed=1)\n",
        "simHash_status = simple_sim_hash.SimpleSimHash(dim=200)\n",
        "\n",
        "print(\"添加训练数据到Status索引...\")\n",
        "start_time = time.time()\n",
        "for img_id, vec in enumerate(train_data_vector):\n",
        "    index_status.add_item_fast10k(vec, id=img_id, lsh=simHash_status, limit=100)\n",
        "status_build_time = time.time() - start_time\n",
        "print(f\"Status索引构建完成，耗时: {status_build_time:.2f}秒\")\n",
        "\n",
        "# 使用FAISS top100结果构建cross distribution边\n",
        "print(\"构建cross distribution边...\")\n",
        "start_time = time.time()\n",
        "for i, query in enumerate(train_queries[:1000]):  # 使用前1000个查询\n",
        "    if i % 100 == 0:\n",
        "        print(f\"  处理查询 {i+1}/1000\")\n",
        "    \n",
        "    if i in train_ground_truth:\n",
        "        faiss_top100 = train_ground_truth[i]\n",
        "        # 在HNSW中搜索这些节点\n",
        "        layer1_nodes = []\n",
        "        for node_id in faiss_top100[:50]:  # 取前50个\n",
        "            if node_id in index_status.items and index_status.items[node_id].level >= 1:\n",
        "                layer1_nodes.append(node_id)\n",
        "        \n",
        "        # 在第1层按照RoarGraph逻辑新增边\n",
        "        if len(layer1_nodes) >= 2:\n",
        "            stats = index_status.build_cross_distribution_edges(\n",
        "                query=query,\n",
        "                top_k=min(10, len(layer1_nodes)),\n",
        "                max_new_edges_per_node=4\n",
        "            )\n",
        "\n",
        "status_edge_time = time.time() - start_time\n",
        "print(f\"Status cross distribution边构建完成，耗时: {status_edge_time:.2f}秒\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3.2 创建High方法索引...\n",
            "添加训练数据到High索引...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "High索引构建完成，耗时: 1350.18秒\n",
            "构建高层边...\n",
            "  处理查询 1/1000\n",
            "  处理查询 101/1000\n",
            "  处理查询 201/1000\n",
            "  处理查询 301/1000\n",
            "  处理查询 401/1000\n",
            "  处理查询 501/1000\n",
            "  处理查询 601/1000\n",
            "  处理查询 701/1000\n",
            "  处理查询 801/1000\n",
            "  处理查询 901/1000\n",
            "High高层边构建完成，耗时: 3.99秒\n"
          ]
        }
      ],
      "source": [
        "# 3.2 High方法 (高层新增边方法)\n",
        "print(\"\\n3.2 创建High方法索引...\")\n",
        "index_high = hnsw_high.HNSWIndex(M=M, ef_construction=ef_construction, ef_search=ef_search, random_seed=1)\n",
        "simHash_high = simple_sim_hash.SimpleSimHash(dim=200)\n",
        "\n",
        "print(\"添加训练数据到High索引...\")\n",
        "start_time = time.time()\n",
        "for img_id, vec in enumerate(train_data_vector):\n",
        "    index_high.add_item_fast10k(vec, id=img_id, lsh=simHash_high, limit=100)\n",
        "high_build_time = time.time() - start_time\n",
        "print(f\"High索引构建完成，耗时: {high_build_time:.2f}秒\")\n",
        "\n",
        "# 使用FAISS top100结果构建高层边\n",
        "print(\"构建高层边...\")\n",
        "start_time = time.time()\n",
        "for i, query in enumerate(train_queries[:1000]):  # 使用前1000个查询\n",
        "    if i % 100 == 0:\n",
        "        print(f\"  处理查询 {i+1}/1000\")\n",
        "    \n",
        "    if i in train_ground_truth:\n",
        "        faiss_top100 = train_ground_truth[i]\n",
        "        # 在HNSW中搜索这些节点到第1层的映射\n",
        "        layer1_nodes = []\n",
        "        for node_id in faiss_top100[:50]:  # 取前50个\n",
        "            if node_id in index_high.items and index_high.items[node_id].level >= 1:\n",
        "                layer1_nodes.append(node_id)\n",
        "        \n",
        "        # 在第1层按照RoarGraph逻辑新增边\n",
        "        if len(layer1_nodes) >= 2:\n",
        "            stats = index_high.build_cross_distribution_edges(\n",
        "                query=query,\n",
        "                top_k=min(10, len(layer1_nodes)),\n",
        "                max_new_edges_per_node=4\n",
        "            )\n",
        "\n",
        "high_edge_time = time.time() - start_time\n",
        "print(f\"High高层边构建完成，耗时: {high_edge_time:.2f}秒\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3.3 创建Norm方法索引...\n",
            "拟合预处理器...\n",
            "Fitting preprocessor on 10000 text samples and 10000 image samples\n",
            "Preprocessor fitting completed\n",
            "预处理器拟合完成，耗时: 0.08秒\n",
            "添加训练数据到Norm索引...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norm索引构建完成，耗时: 3554.47秒\n"
          ]
        }
      ],
      "source": [
        "# 3.3 Norm方法 (高维映射方法)\n",
        "print(\"\\n3.3 创建Norm方法索引...\")\n",
        "# 创建数据预处理器\n",
        "preprocessor = hnsw_norm.DataPreprocessor(\n",
        "    use_pca=True,\n",
        "    n_components=200,\n",
        "    use_global_whitening=True,\n",
        "    sub_modality_scaling=True\n",
        ")\n",
        "\n",
        "print(\"拟合预处理器...\")\n",
        "start_time = time.time()\n",
        "preprocessor.fit(train_data_vector, train_data_vector, sample_size=10000)\n",
        "norm_preprocess_time = time.time() - start_time\n",
        "print(f\"预处理器拟合完成，耗时: {norm_preprocess_time:.2f}秒\")\n",
        "\n",
        "# 创建HNSW索引\n",
        "index_norm = hnsw_norm.HNSWIndex(\n",
        "    M=M,\n",
        "    ef_construction=ef_construction,\n",
        "    ef_search=ef_search,\n",
        "    preprocessor=preprocessor\n",
        ")\n",
        "\n",
        "print(\"添加训练数据到Norm索引...\")\n",
        "start_time = time.time()\n",
        "for i, vec in enumerate(train_data_vector):\n",
        "    index_norm.add_item(vec, id=i)\n",
        "norm_build_time = time.time() - start_time\n",
        "print(f\"Norm索引构建完成，耗时: {norm_build_time:.2f}秒\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 索引构建总结 ===\n",
            "Status方法: 构建1192.35s + 边构建0.14s = 1192.49s\n",
            "High方法: 构建1136.47s + 边构建2.59s = 1139.06s\n",
            "Norm方法: 预处理0.08s + 构建3554.47s = 3554.55s\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n=== 索引构建总结 ===\")\n",
        "print(f\"Status方法: 构建{status_build_time:.2f}s + 边构建{status_edge_time:.2f}s = {status_build_time + status_edge_time:.2f}s\")\n",
        "print(f\"High方法: 构建{high_build_time:.2f}s + 边构建{high_edge_time:.2f}s = {high_build_time + high_edge_time:.2f}s\")\n",
        "print(f\"Norm方法: 预处理{norm_preprocess_time:.2f}s + 构建{norm_build_time:.2f}s = {norm_preprocess_time + norm_build_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 4. Recall性能测试 (ef_search=32) ===\n",
            "测试Status方法...\n",
            "测试Status方法 (ef_search=32)...\n",
            "  搜索结果: [484776, 357178, 309679, 493505, 322198, 472983, 478010, 357973, 266942, 483270, 461479, 475748, 412971, 289833, 273606, 291312, 278630, 367879, 357418, 303458, 490132, 460651, 331321, 366391, 326373, 329278, 405708, 342168, 276840, 435866, 363273, 483043, 376521, 283668, 472399, 359568, 476681, 360644, 316593, 273807, 357954, 294527, 391355, 373303, 361546, 490705, 363068, 362327, 367618, 393743, 471253, 273391, 399451, 378149, 359549, 356723, 477948, 304383, 419289, 329015, 460955, 414921, 498991, 316207, 316147, 361020, 305214, 342729, 335296, 354560, 427552, 362675, 360582, 417235, 372257, 343123, 359575, 356893, 330547, 360570, 356601, 492555, 458415, 483098, 484072, 434699, 341061, 320019, 360337, 498503, 316458, 355158, 361940, 496470, 358447, 378796, 279808, 361141, 357918, 335179]\n",
            "  真实结果: [450938, 433164, 273168, 197768, 446539, 114597, 166119, 351005, 297013, 387034, 118529, 270950, 443415, 475354, 119404, 481549, 348835, 351111, 380869, 354360, 46775, 104998, 57946, 222629, 46956, 26165, 327783, 142800, 498687, 454881, 417378, 15560, 396110, 241241, 392323, 365809, 439516, 141796, 482766, 74151, 478561, 161810, 115050, 30807, 65268, 26159, 76581, 313282, 99660, 496140, 91828, 270484, 307967, 339310, 72767, 76276, 35872, 351813, 342630, 6940, 187681, 218782, 453213, 316839, 498876, 305505, 443486, 77713, 355767, 423349, 20107, 387996, 158089, 58428, 96610, 133870, 11408, 364752, 142080, 328994, 366763, 91908, 431275, 241251, 309460, 465622, 71743, 314026, 495976, 312480, 497829, 474173, 4450, 153810, 133791, 441544, 92647, 86201, 322329, 162520]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "测试High方法...\n",
            "测试High方法 (ef_search=32)...\n",
            "  搜索结果: [484776, 357178, 309679, 493505, 322198, 472983, 478010, 357973, 266942, 483270, 461479, 475748, 412971, 289833, 273606, 291312, 278630, 367879, 357418, 303458, 490132, 460651, 331321, 366391, 326373, 329278, 405708, 342168, 276840, 435866, 363273, 483043, 376521, 283668, 472399, 359568, 476681, 360644, 316593, 273807, 357954, 294527, 391355, 373303, 361546, 490705, 363068, 362327, 367618, 393743, 471253, 273391, 399451, 378149, 359549, 356723, 477948, 304383, 419289, 329015, 460955, 414921, 498991, 316207, 316147, 361020, 305214, 342729, 335296, 354560, 427552, 362675, 360582, 417235, 372257, 343123, 359575, 356893, 330547, 360570, 356601, 492555, 458415, 483098, 484072, 434699, 341061, 320019, 360337, 498503, 316458, 355158, 361940, 496470, 358447, 378796, 279808, 361141, 357918, 335179]\n",
            "  真实结果: [450938, 433164, 273168, 197768, 446539, 114597, 166119, 351005, 297013, 387034, 118529, 270950, 443415, 475354, 119404, 481549, 348835, 351111, 380869, 354360, 46775, 104998, 57946, 222629, 46956, 26165, 327783, 142800, 498687, 454881, 417378, 15560, 396110, 241241, 392323, 365809, 439516, 141796, 482766, 74151, 478561, 161810, 115050, 30807, 65268, 26159, 76581, 313282, 99660, 496140, 91828, 270484, 307967, 339310, 72767, 76276, 35872, 351813, 342630, 6940, 187681, 218782, 453213, 316839, 498876, 305505, 443486, 77713, 355767, 423349, 20107, 387996, 158089, 58428, 96610, 133870, 11408, 364752, 142080, 328994, 366763, 91908, 431275, 241251, 309460, 465622, 71743, 314026, 495976, 312480, 497829, 474173, 4450, 153810, 133791, 441544, 92647, 86201, 322329, 162520]\n",
            "测试Norm方法...\n",
            "测试Norm方法 (ef_search=32)...\n",
            "  搜索结果: [494527, 383056, 458723, 473623, 364484, 365512, 367980, 409231, 393129, 373739, 411911, 414212, 443091, 464057, 430340, 414809, 413230, 374599, 408148, 416329, 494841, 435994, 367712, 462535, 424989, 390147, 420044, 382306, 445797, 431845, 448710, 454683, 457133, 456298, 374696, 464490, 427461, 408535, 378171, 369390, 457160, 398134, 396078, 407194, 383929, 419023, 464442, 464199, 432951, 376771, 442521, 427806, 476675, 436935, 415882, 442732, 494001, 398806, 421240, 380326, 457638, 371835, 497634, 492072, 433585, 388948, 387099, 421922, 379093, 424676, 412963, 428805, 444342, 441479, 379290, 387339, 363623, 440806, 382100, 491165, 448453, 451875, 400759, 427594, 443717, 413651, 432838, 391771, 407780, 414611, 381154, 371545, 417940, 405955, 409454, 481870, 486988, 388491, 443325, 396061]\n",
            "  真实结果: [450938, 433164, 273168, 197768, 446539, 114597, 166119, 351005, 297013, 387034, 118529, 270950, 443415, 475354, 119404, 481549, 348835, 351111, 380869, 354360, 46775, 104998, 57946, 222629, 46956, 26165, 327783, 142800, 498687, 454881, 417378, 15560, 396110, 241241, 392323, 365809, 439516, 141796, 482766, 74151, 478561, 161810, 115050, 30807, 65268, 26159, 76581, 313282, 99660, 496140, 91828, 270484, 307967, 339310, 72767, 76276, 35872, 351813, 342630, 6940, 187681, 218782, 453213, 316839, 498876, 305505, 443486, 77713, 355767, 423349, 20107, 387996, 158089, 58428, 96610, 133870, 11408, 364752, 142080, 328994, 366763, 91908, 431275, 241251, 309460, 465622, 71743, 314026, 495976, 312480, 497829, 474173, 4450, 153810, 133791, 441544, 92647, 86201, 322329, 162520]\n",
            "\n",
            "=== Recall性能测试结果 ===\n",
            "方法         Mean Recall  Mean Steps   Mean Time(ms)  \n",
            "-------------------------------------------------------\n",
            "Status     0.012        324.2        1.54           \n",
            "High       0.012        322.9        1.62           \n",
            "Norm       0.017        107.9        0.79           \n"
          ]
        }
      ],
      "source": [
        "# 4. Recall性能测试 (ef_search=32)\n",
        "print(\"\\n=== 4. Recall性能测试 (ef_search=32) ===\")\n",
        "\n",
        "def test_method_recall(index, queries, ground_truths, method_name, ef_search=32):\n",
        "    \"\"\"测试方法的recall性能\"\"\"\n",
        "    recalls = []\n",
        "    search_times = []\n",
        "    search_steps_list = []\n",
        "    \n",
        "    print(f\"测试{method_name}方法 (ef_search={ef_search})...\")\n",
        "    for i, query in enumerate(queries):\n",
        "        # if i % 100 == 0:\n",
        "        #     print(f\"  处理查询 {i+1}/{len(queries)}\")\n",
        "        \n",
        "        if i not in ground_truths:\n",
        "            continue\n",
        "            \n",
        "        # 搜索\n",
        "        start_time = time.time()\n",
        "        results, search_steps = index.query_with_steps(query, k=100, ef=ef_search)\n",
        "        search_time = time.time() - start_time\n",
        "        if i == 0:\n",
        "            print(f\"  搜索结果: {results}\")\n",
        "            print(f\"  真实结果: {ground_truths[i]}\")\n",
        "        \n",
        "        # 计算recall@100\n",
        "        recall = calculate_recall_at_k(results, ground_truths[i], 100)\n",
        "        \n",
        "        recalls.append(recall)\n",
        "        search_times.append(search_time)\n",
        "        search_steps_list.append(search_steps)\n",
        "    \n",
        "    return {\n",
        "        'mean_recall': np.mean(recalls),\n",
        "        'std_recall': np.std(recalls),\n",
        "        'mean_steps': np.mean(search_steps_list),\n",
        "        'std_steps': np.std(search_steps_list),\n",
        "        'mean_time': np.mean(search_times),\n",
        "        'std_time': np.std(search_times),\n",
        "        'recalls': recalls,\n",
        "        'search_steps': search_steps_list,\n",
        "        'search_times': search_times\n",
        "    }\n",
        "\n",
        "# 测试三种方法\n",
        "print(\"测试Status方法...\")\n",
        "status_results = test_method_recall(index_status, test_queries_final, test_ground_truth, \"Status\", ef_search=32)\n",
        "\n",
        "print(\"测试High方法...\")\n",
        "high_results = test_method_recall(index_high, test_queries_final, test_ground_truth, \"High\", ef_search=32)\n",
        "\n",
        "print(\"测试Norm方法...\")\n",
        "norm_results = test_method_recall(index_norm, test_queries_final, test_ground_truth, \"Norm\", ef_search=32)\n",
        "\n",
        "# 显示结果\n",
        "print(f\"\\n=== Recall性能测试结果 ===\")\n",
        "print(f\"{'方法':<10} {'Mean Recall':<12} {'Mean Steps':<12} {'Mean Time(ms)':<15}\")\n",
        "print(\"-\" * 55)\n",
        "print(f\"{'Status':<10} {status_results['mean_recall']:<12.3f} {status_results['mean_steps']:<12.1f} {status_results['mean_time']*1000:<15.2f}\")\n",
        "print(f\"{'High':<10} {high_results['mean_recall']:<12.3f} {high_results['mean_steps']:<12.1f} {high_results['mean_time']*1000:<15.2f}\")\n",
        "print(f\"{'Norm':<10} {norm_results['mean_recall']:<12.3f} {norm_results['mean_steps']:<12.1f} {norm_results['mean_time']*1000:<15.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 5. Effort分位数测试 (Recall90下的搜索步长) ===\n",
            "测试effort分位数对应的查询...\n",
            "\n",
            "=== Recall90下的Effort分位数步长汇总 ===\n",
            "Percentile   Status Steps    High Steps      Norm Steps     \n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# 5. Effort分位数测试 (Recall90下的搜索步长)\n",
        "print(\"\\n=== 5. Effort分位数测试 (Recall90下的搜索步长) ===\")\n",
        "\n",
        "def find_ef_for_recall90(index, query, ground_truth, k=100):\n",
        "    \"\"\"找到达到recall90所需的最小ef_search值\"\"\"\n",
        "    for ef in [16, 32, 64, 128, 256, 512]:\n",
        "        results, steps = index.query_with_steps(query, k=k, ef=ef)\n",
        "        recall = calculate_recall_at_k(results, ground_truth, k)\n",
        "        if recall >= 0.90:\n",
        "            return ef, recall, steps\n",
        "    return 512, 0.0, 0\n",
        "\n",
        "# 测试effort分位数对应的query\n",
        "effort_results = {}\n",
        "print(\"测试effort分位数对应的查询...\")\n",
        "\n",
        "for percentile, info in effort_percentiles.items():\n",
        "    query_id = info['query_id']\n",
        "    \n",
        "    # 检查query_id是否在测试范围内\n",
        "    if query_id < len(test_queries_final):\n",
        "        query = test_queries_final[query_id]\n",
        "        \n",
        "        if query_id in test_ground_truth:\n",
        "            gt = test_ground_truth[query_id]\n",
        "            \n",
        "            print(f\"测试P{percentile} (query_id={query_id})...\")\n",
        "            \n",
        "            # 测试三种方法\n",
        "            ef_status, recall_status, steps_status = find_ef_for_recall90(index_status, query, gt)\n",
        "            ef_high, recall_high, steps_high = find_ef_for_recall90(index_high, query, gt)\n",
        "            ef_norm, recall_norm, steps_norm = find_ef_for_recall90(index_norm, query, gt)\n",
        "            \n",
        "            effort_results[percentile] = {\n",
        "                'query_id': query_id,\n",
        "                'status': {'ef': ef_status, 'recall': recall_status, 'steps': steps_status},\n",
        "                'high': {'ef': ef_high, 'recall': recall_high, 'steps': steps_high},\n",
        "                'norm': {'ef': ef_norm, 'recall': recall_norm, 'steps': steps_norm}\n",
        "            }\n",
        "            \n",
        "            print(f\"  Status: ef={ef_status}, recall={recall_status:.3f}, steps={steps_status}\")\n",
        "            print(f\"  High: ef={ef_high}, recall={recall_high:.3f}, steps={steps_high}\")\n",
        "            print(f\"  Norm: ef={ef_norm}, recall={recall_norm:.3f}, steps={steps_norm}\")\n",
        "\n",
        "# 显示结果汇总\n",
        "print(f\"\\n=== Recall90下的Effort分位数步长汇总 ===\")\n",
        "print(f\"{'Percentile':<12} {'Status Steps':<15} {'High Steps':<15} {'Norm Steps':<15}\")\n",
        "print(\"-\" * 60)\n",
        "for percentile, results in effort_results.items():\n",
        "    print(f\"P{percentile:<10} {results['status']['steps']:<15.1f} {results['high']['steps']:<15.1f} {results['norm']['steps']:<15.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. 结果可视化\n",
        "print(\"\\n=== 6. 结果可视化 ===\")\n",
        "\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# 子图1: Recall对比\n",
        "plt.subplot(2, 3, 1)\n",
        "methods = ['Status', 'High', 'Norm']\n",
        "recalls = [status_results['mean_recall'], high_results['mean_recall'], norm_results['mean_recall']]\n",
        "recall_stds = [status_results['std_recall'], high_results['std_recall'], norm_results['std_recall']]\n",
        "\n",
        "plt.bar(methods, recalls, yerr=recall_stds, capsize=5, alpha=0.7)\n",
        "plt.ylabel('Mean Recall@100')\n",
        "plt.title('Recall性能对比 (ef_search=32)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 子图2: 搜索步数对比\n",
        "plt.subplot(2, 3, 2)\n",
        "steps = [status_results['mean_steps'], high_results['mean_steps'], norm_results['mean_steps']]\n",
        "steps_stds = [status_results['std_steps'], high_results['std_steps'], norm_results['std_steps']]\n",
        "\n",
        "plt.bar(methods, steps, yerr=steps_stds, capsize=5, alpha=0.7, color='orange')\n",
        "plt.ylabel('Mean Search Steps')\n",
        "plt.title('搜索步数对比 (ef_search=32)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 子图3: 搜索时间对比\n",
        "plt.subplot(2, 3, 3)\n",
        "times = [status_results['mean_time']*1000, high_results['mean_time']*1000, norm_results['mean_time']*1000]\n",
        "times_stds = [status_results['std_time']*1000, high_results['std_time']*1000, norm_results['std_time']*1000]\n",
        "\n",
        "plt.bar(methods, times, yerr=times_stds, capsize=5, alpha=0.7, color='green')\n",
        "plt.ylabel('Mean Search Time (ms)')\n",
        "plt.title('搜索时间对比 (ef_search=32)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 子图4: Recall90下的搜索步数对比\n",
        "plt.subplot(2, 3, 4)\n",
        "percentiles = list(effort_results.keys())\n",
        "status_steps = [effort_results[p]['status']['steps'] for p in percentiles]\n",
        "high_steps = [effort_results[p]['high']['steps'] for p in percentiles]\n",
        "norm_steps = [effort_results[p]['norm']['steps'] for p in percentiles]\n",
        "\n",
        "x = np.arange(len(percentiles))\n",
        "width = 0.25\n",
        "\n",
        "plt.bar(x - width, status_steps, width, label='Status', alpha=0.7)\n",
        "plt.bar(x, high_steps, width, label='High', alpha=0.7)\n",
        "plt.bar(x + width, norm_steps, width, label='Norm', alpha=0.7)\n",
        "\n",
        "plt.xlabel('Effort Percentile')\n",
        "plt.ylabel('Search Steps (Recall90)')\n",
        "plt.title('Recall90下的搜索步数对比')\n",
        "plt.xticks(x, [f'P{p}' for p in percentiles])\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 子图5: Recall分布对比\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.hist(status_results['recalls'], bins=20, alpha=0.5, label='Status', density=True)\n",
        "plt.hist(high_results['recalls'], bins=20, alpha=0.5, label='High', density=True)\n",
        "plt.hist(norm_results['recalls'], bins=20, alpha=0.5, label='Norm', density=True)\n",
        "plt.xlabel('Recall@100')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Recall分布对比')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 子图6: 搜索步数分布对比\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.hist(status_results['search_steps'], bins=20, alpha=0.5, label='Status', density=True)\n",
        "plt.hist(high_results['search_steps'], bins=20, alpha=0.5, label='High', density=True)\n",
        "plt.hist(norm_results['search_steps'], bins=20, alpha=0.5, label='Norm', density=True)\n",
        "plt.xlabel('Search Steps')\n",
        "plt.ylabel('Density')\n",
        "plt.title('搜索步数分布对比')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. 保存最终结果\n",
        "print(\"\\n=== 7. 保存最终结果 ===\")\n",
        "\n",
        "# 准备保存的数据\n",
        "final_results = {\n",
        "    'experiment_info': {\n",
        "        'description': 'HNSW方法对比测试',\n",
        "        'dataset': 'Text2Image 500K vectors',\n",
        "        'test_queries': len(test_queries_final),\n",
        "        'train_queries': len(train_queries),\n",
        "        'parameters': {\n",
        "            'M': M,\n",
        "            'ef_construction': ef_construction,\n",
        "            'ef_search': ef_search\n",
        "        }\n",
        "    },\n",
        "    'build_times': {\n",
        "        'status': status_build_time + status_edge_time,\n",
        "        'high': high_build_time + high_edge_time,\n",
        "        'norm': norm_preprocess_time + norm_build_time\n",
        "    },\n",
        "    'recall_results': {\n",
        "        'status': {\n",
        "            'mean_recall': float(status_results['mean_recall']),\n",
        "            'std_recall': float(status_results['std_recall']),\n",
        "            'mean_steps': float(status_results['mean_steps']),\n",
        "            'std_steps': float(status_results['std_steps']),\n",
        "            'mean_time_ms': float(status_results['mean_time'] * 1000),\n",
        "            'std_time_ms': float(status_results['std_time'] * 1000)\n",
        "        },\n",
        "        'high': {\n",
        "            'mean_recall': float(high_results['mean_recall']),\n",
        "            'std_recall': float(high_results['std_recall']),\n",
        "            'mean_steps': float(high_results['mean_steps']),\n",
        "            'std_steps': float(high_results['std_steps']),\n",
        "            'mean_time_ms': float(high_results['mean_time'] * 1000),\n",
        "            'std_time_ms': float(high_results['std_time'] * 1000)\n",
        "        },\n",
        "        'norm': {\n",
        "            'mean_recall': float(norm_results['mean_recall']),\n",
        "            'std_recall': float(norm_results['std_recall']),\n",
        "            'mean_steps': float(norm_results['mean_steps']),\n",
        "            'std_steps': float(norm_results['std_steps']),\n",
        "            'mean_time_ms': float(norm_results['mean_time'] * 1000),\n",
        "            'std_time_ms': float(norm_results['std_time'] * 1000)\n",
        "        }\n",
        "    },\n",
        "    'effort_percentiles': effort_results\n",
        "}\n",
        "\n",
        "# 保存到JSON文件\n",
        "results_file = '/root/code/vectordbindexing/hnsw_methods_comparison_results.json'\n",
        "with open(results_file, 'w') as f:\n",
        "    json.dump(final_results, f, indent=2)\n",
        "\n",
        "print(f\"结果已保存到: {results_file}\")\n",
        "\n",
        "# 总结报告\n",
        "print(f\"\\n=== HNSW方法对比测试总结 ===\")\n",
        "print(f\"1. 实验设置:\")\n",
        "print(f\"   - 数据集: {final_results['experiment_info']['dataset']}\")\n",
        "print(f\"   - 训练查询: {final_results['experiment_info']['train_queries']} 个\")\n",
        "print(f\"   - 测试查询: {final_results['experiment_info']['test_queries']} 个\")\n",
        "print(f\"   - 参数: M={M}, ef_construction={ef_construction}, ef_search={ef_search}\")\n",
        "\n",
        "print(f\"\\n2. 构建时间对比:\")\n",
        "print(f\"   - Status方法: {final_results['build_times']['status']:.2f}秒\")\n",
        "print(f\"   - High方法: {final_results['build_times']['high']:.2f}秒\")\n",
        "print(f\"   - Norm方法: {final_results['build_times']['norm']:.2f}秒\")\n",
        "\n",
        "print(f\"\\n3. Recall性能对比 (ef_search=32):\")\n",
        "print(f\"   - Status方法: {final_results['recall_results']['status']['mean_recall']:.3f} ± {final_results['recall_results']['status']['std_recall']:.3f}\")\n",
        "print(f\"   - High方法: {final_results['recall_results']['high']['mean_recall']:.3f} ± {final_results['recall_results']['high']['std_recall']:.3f}\")\n",
        "print(f\"   - Norm方法: {final_results['recall_results']['norm']['mean_recall']:.3f} ± {final_results['recall_results']['norm']['std_recall']:.3f}\")\n",
        "\n",
        "print(f\"\\n4. 搜索步数对比 (ef_search=32):\")\n",
        "print(f\"   - Status方法: {final_results['recall_results']['status']['mean_steps']:.1f} ± {final_results['recall_results']['status']['std_steps']:.1f}\")\n",
        "print(f\"   - High方法: {final_results['recall_results']['high']['mean_steps']:.1f} ± {final_results['recall_results']['high']['std_steps']:.1f}\")\n",
        "print(f\"   - Norm方法: {final_results['recall_results']['norm']['mean_steps']:.1f} ± {final_results['recall_results']['norm']['std_steps']:.1f}\")\n",
        "\n",
        "print(f\"\\n5. 搜索时间对比 (ef_search=32):\")\n",
        "print(f\"   - Status方法: {final_results['recall_results']['status']['mean_time_ms']:.2f} ± {final_results['recall_results']['status']['std_time_ms']:.2f}ms\")\n",
        "print(f\"   - High方法: {final_results['recall_results']['high']['mean_time_ms']:.2f} ± {final_results['recall_results']['high']['std_time_ms']:.2f}ms\")\n",
        "print(f\"   - Norm方法: {final_results['recall_results']['norm']['mean_time_ms']:.2f} ± {final_results['recall_results']['norm']['std_time_ms']:.2f}ms\")\n",
        "\n",
        "print(f\"\\n6. 关键发现:\")\n",
        "best_recall_method = max(['status', 'high', 'norm'], key=lambda x: final_results['recall_results'][x]['mean_recall'])\n",
        "fastest_method = min(['status', 'high', 'norm'], key=lambda x: final_results['recall_results'][x]['mean_time_ms'])\n",
        "most_efficient_method = min(['status', 'high', 'norm'], key=lambda x: final_results['recall_results'][x]['mean_steps'])\n",
        "\n",
        "print(f\"   - 最高Recall: {best_recall_method.title()}方法\")\n",
        "print(f\"   - 最快搜索: {fastest_method.title()}方法\")\n",
        "print(f\"   - 最少步数: {most_efficient_method.title()}方法\")\n",
        "\n",
        "print(f\"\\n✅ HNSW方法对比测试完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 三种HNSW方法对比测试框架 ===\n",
        "print(\"=== 三种HNSW方法对比测试框架 ===\")\n",
        "\n",
        "from io_utils import read_fbin, read_ibin\n",
        "import faiss\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 导入三种HNSW方法\n",
        "import hnsw_cosine_status as hnsw_status\n",
        "import hnsw_cosine_status_high as hnsw_high\n",
        "import simple_sim_hash\n",
        "\n",
        "print(faiss.__version__)\n",
        "\n",
        "# 数据路径\n",
        "file_path = \"/root/code/vectordbindexing/Text2Image/base.1M.fbin\"\n",
        "query_path = \"/root/code/vectordbindexing/Text2Image/query.public.100K.fbin\"\n",
        "\n",
        "# 读取数据集\n",
        "print(\"\\n=== 读取数据集 ===\")\n",
        "print(\"读取图像向量...\")\n",
        "data_vector = read_fbin(file_path)\n",
        "print(f\"图像向量: {data_vector.shape}, dtype: {data_vector.dtype}\")\n",
        "\n",
        "print(\"读取查询向量...\")\n",
        "query_vector = read_fbin(query_path)\n",
        "print(f\"查询向量: {query_vector.shape}, dtype: {query_vector.dtype}\")\n",
        "\n",
        "# 使用前100K数据进行测试\n",
        "train_data_vector = data_vector[:100000]\n",
        "print(f\"使用训练数据: {train_data_vector.shape}\")\n",
        "\n",
        "# 数据分割：9/10训练，1/10测试\n",
        "total_queries = len(query_vector)\n",
        "train_query_size = int(total_queries * 0.9)\n",
        "test_query_size = total_queries - train_query_size\n",
        "\n",
        "train_queries = query_vector[:train_query_size]\n",
        "test_queries = query_vector[train_query_size:]\n",
        "\n",
        "print(f\"\\n数据分割:\")\n",
        "print(f\"  总查询数: {total_queries}\")\n",
        "print(f\"  训练查询数: {train_query_size}\")\n",
        "print(f\"  测试查询数: {test_query_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 步骤1: 创建FAISS HNSW索引作为Baseline ===\n",
        "print(\"\\n=== 步骤1: 创建FAISS HNSW索引作为Baseline ===\")\n",
        "\n",
        "# 创建FAISS HNSW索引\n",
        "dimension = 200\n",
        "faiss_index = faiss.IndexHNSWFlat(dimension, 32)  # M=32\n",
        "\n",
        "# 添加数据到FAISS索引\n",
        "print(\"添加数据到FAISS索引...\")\n",
        "start_time = time.time()\n",
        "faiss_index.add(train_data_vector.astype('float32'))\n",
        "build_time = time.time() - start_time\n",
        "print(f\"FAISS索引构建完成，耗时: {build_time:.2f}秒\")\n",
        "print(f\"索引大小: {faiss_index.ntotal} 个向量\")\n",
        "\n",
        "# 设置搜索参数（使用较宽的beam width获得更准确的结果）\n",
        "faiss_index.hnsw.efSearch = 512  # 使用较大的efSearch\n",
        "print(f\"FAISS搜索参数: efSearch={faiss_index.hnsw.efSearch}\")\n",
        "\n",
        "# 对训练查询进行FAISS搜索，生成ground truth\n",
        "print(\"\\n对训练查询进行FAISS搜索...\")\n",
        "faiss_ground_truth = {}\n",
        "faiss_search_efforts = []\n",
        "\n",
        "for i, query in enumerate(train_queries):\n",
        "    start_time = time.time()\n",
        "    # 搜索top100\n",
        "    distances, indices = faiss_index.search(query.reshape(1, -1).astype('float32'), 100)\n",
        "    search_time = time.time() - start_time\n",
        "    \n",
        "    faiss_ground_truth[i] = indices[0].tolist()\n",
        "    faiss_search_efforts.append(search_time * 1000)  # 转换为毫秒\n",
        "    \n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"  完成 {i+1}/{train_query_size} 个查询\")\n",
        "\n",
        "print(f\"\\nFAISS搜索统计:\")\n",
        "print(f\"平均搜索时间: {np.mean(faiss_search_efforts):.2f}ms\")\n",
        "print(f\"搜索时间标准差: {np.std(faiss_search_efforts):.2f}ms\")\n",
        "\n",
        "# 计算Effort的P10-P99分位数，并记录对应的query id\n",
        "print(\"\\n=== 计算Effort分位数 ===\")\n",
        "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
        "effort_percentiles = {}\n",
        "query_ids_by_percentile = {}\n",
        "\n",
        "for p in percentiles:\n",
        "    percentile_value = np.percentile(faiss_search_efforts, p)\n",
        "    effort_percentiles[p] = percentile_value\n",
        "    \n",
        "    # 找到effort接近该分位数的query id\n",
        "    effort_array = np.array(faiss_search_efforts)\n",
        "    closest_idx = np.argmin(np.abs(effort_array - percentile_value))\n",
        "    query_ids_by_percentile[p] = closest_idx\n",
        "    \n",
        "    print(f\"P{p}: {percentile_value:.2f}ms, Query ID: {closest_idx}\")\n",
        "\n",
        "# 保存结果到文件\n",
        "print(\"\\n=== 保存Ground Truth和Effort数据 ===\")\n",
        "ground_truth_file = \"/root/code/vectordbindexing/faiss_ground_truth.json\"\n",
        "with open(ground_truth_file, 'w') as f:\n",
        "    json.dump(faiss_ground_truth, f, indent=2)\n",
        "print(f\"Ground Truth已保存到: {ground_truth_file}\")\n",
        "\n",
        "effort_data = {\n",
        "    'effort_percentiles': effort_percentiles,\n",
        "    'query_ids_by_percentile': query_ids_by_percentile,\n",
        "    'all_efforts': faiss_search_efforts,\n",
        "    'statistics': {\n",
        "        'mean': np.mean(faiss_search_efforts),\n",
        "        'std': np.std(faiss_search_efforts),\n",
        "        'min': np.min(faiss_search_efforts),\n",
        "        'max': np.max(faiss_search_efforts)\n",
        "    }\n",
        "}\n",
        "\n",
        "effort_file = \"/root/code/vectordbindexing/faiss_effort_analysis.json\"\n",
        "with open(effort_file, 'w') as f:\n",
        "    json.dump(effort_data, f, indent=2)\n",
        "print(f\"Effort分析已保存到: {effort_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 步骤2: 创建三种HNSW索引 ===\n",
        "print(\"\\n=== 步骤2: 创建三种HNSW索引 ===\")\n",
        "\n",
        "# 通用参数\n",
        "M = 64\n",
        "ef_construction = 128\n",
        "ef_search = 32  # 测试时使用ef_search=32\n",
        "random_seed = 1\n",
        "\n",
        "# 创建LSH对象\n",
        "simHash = simple_sim_hash.SimpleSimHash(dim=200)\n",
        "\n",
        "# 1. Status方法 (RoarGraph)\n",
        "print(\"\\n--- 创建Status方法 (RoarGraph) ---\")\n",
        "start_time = time.time()\n",
        "status_index = hnsw_status.HNSWIndex(M=M, ef_construction=ef_construction, ef_search=ef_search, random_seed=random_seed)\n",
        "\n",
        "# 添加训练数据\n",
        "for img_id, vec in enumerate(train_data_vector):\n",
        "    status_index.add_item_fast10k(vec, lsh=simHash, limit=100)\n",
        "\n",
        "# 添加训练查询\n",
        "for qid, vec in enumerate(train_queries):\n",
        "    status_index.add_item_fast10k(vec, lsh=simHash, limit=100)\n",
        "\n",
        "status_build_time = time.time() - start_time\n",
        "print(f\"Status索引构建完成，耗时: {status_build_time:.2f}秒\")\n",
        "print(f\"Status索引大小: {len(status_index.items)} 个向量\")\n",
        "\n",
        "# 2. High方法 (高层新增边) - 需要修改逻辑\n",
        "print(\"\\n--- 创建High方法 (高层新增边) ---\")\n",
        "start_time = time.time()\n",
        "high_index = hnsw_high.HNSWIndex(M=M, ef_construction=ef_construction, ef_search=ef_search, random_seed=random_seed)\n",
        "\n",
        "# 添加训练数据\n",
        "for img_id, vec in enumerate(train_data_vector):\n",
        "    high_index.add_item_fast10k(vec, lsh=simHash, limit=100)\n",
        "\n",
        "# 添加训练查询\n",
        "for qid, vec in enumerate(train_queries):\n",
        "    high_index.add_item_fast10k(vec, lsh=simHash, limit=100)\n",
        "\n",
        "high_build_time = time.time() - start_time\n",
        "print(f\"High索引构建完成，耗时: {high_build_time:.2f}秒\")\n",
        "print(f\"High索引大小: {len(high_index.items)} 个向量\")\n",
        "\n",
        "# 3. Norm方法 (高维映射) - 暂时使用相同的索引，后续可以扩展\n",
        "print(\"\\n--- 创建Norm方法 (高维映射) ---\")\n",
        "# 注意：这里暂时使用相同的实现，实际应用中需要实现高维映射逻辑\n",
        "norm_index = hnsw_status.HNSWIndex(M=M, ef_construction=ef_construction, ef_search=ef_search, random_seed=random_seed)\n",
        "\n",
        "# 添加训练数据\n",
        "for img_id, vec in enumerate(train_data_vector):\n",
        "    norm_index.add_item_fast10k(vec, lsh=simHash, limit=100)\n",
        "\n",
        "# 添加训练查询\n",
        "for qid, vec in enumerate(train_queries):\n",
        "    norm_index.add_item_fast10k(vec, lsh=simHash, limit=100)\n",
        "\n",
        "norm_build_time = time.time() - start_time\n",
        "print(f\"Norm索引构建完成，耗时: {norm_build_time:.2f}秒\")\n",
        "print(f\"Norm索引大小: {len(norm_index.items)} 个向量\")\n",
        "\n",
        "print(f\"\\n=== 索引构建统计 ===\")\n",
        "print(f\"Status方法: {status_build_time:.2f}秒\")\n",
        "print(f\"High方法: {high_build_time:.2f}秒\")\n",
        "print(f\"Norm方法: {norm_build_time:.2f}秒\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 步骤3: 测试三种方法的Recall性能 ===\n",
        "print(\"\\n=== 步骤3: 测试三种方法的Recall性能 ===\")\n",
        "\n",
        "def calculate_recall_at_k(predicted_ids, ground_truth_ids, k):\n",
        "    \"\"\"\n",
        "    计算recall@k\n",
        "    \n",
        "    Args:\n",
        "        predicted_ids: 预测的top-k结果\n",
        "        ground_truth_ids: ground truth结果\n",
        "        k: top-k值\n",
        "    \n",
        "    Returns:\n",
        "        recall@k值\n",
        "    \"\"\"\n",
        "    # 取前k个预测结果\n",
        "    top_k_pred = set(predicted_ids[:k])\n",
        "    \n",
        "    # ground truth已经是正确的索引\n",
        "    valid_gt = set(ground_truth_ids)\n",
        "    \n",
        "    if len(valid_gt) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # 计算交集\n",
        "    intersection = top_k_pred.intersection(valid_gt)\n",
        "    \n",
        "    # recall@k = |intersection| / |ground_truth|\n",
        "    recall = len(intersection) / len(valid_gt)\n",
        "    return recall\n",
        "\n",
        "# 测试三种方法的recall性能\n",
        "methods = {\n",
        "    'Status (RoarGraph)': status_index,\n",
        "    'High (高层新增边)': high_index,\n",
        "    'Norm (高维映射)': norm_index\n",
        "}\n",
        "\n",
        "recall_results = {}\n",
        "search_time_results = {}\n",
        "\n",
        "# 使用测试集 (后1/10的查询) 进行测试\n",
        "test_sample_size = min(1000, len(test_queries))  # 使用1000个测试查询\n",
        "test_sample_queries = test_queries[:test_sample_size]\n",
        "\n",
        "print(f\"使用 {test_sample_size} 个测试查询进行recall测试...\")\n",
        "\n",
        "for method_name, index in methods.items():\n",
        "    print(f\"\\n--- 测试 {method_name} ---\")\n",
        "    \n",
        "    recalls = []\n",
        "    search_times = []\n",
        "    \n",
        "    for i, query in enumerate(test_sample_queries):\n",
        "        # 获取对应的ground truth (需要调整索引)\n",
        "        gt_query_id = train_query_size + i\n",
        "        ground_truth_ids = faiss_ground_truth.get(str(gt_query_id % train_query_size), [])\n",
        "        \n",
        "        # 搜索\n",
        "        start_time = time.time()\n",
        "        results = index.query(query, k=100, ef=ef_search)\n",
        "        search_time = time.time() - start_time\n",
        "        \n",
        "        # 计算recall@100\n",
        "        recall = calculate_recall_at_k(results, ground_truth_ids, 100)\n",
        "        \n",
        "        recalls.append(recall)\n",
        "        search_times.append(search_time * 1000)  # 转换为毫秒\n",
        "        \n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"  完成 {i+1}/{test_sample_size} 个查询\")\n",
        "    \n",
        "    # 统计结果\n",
        "    recall_results[method_name] = {\n",
        "        'mean': np.mean(recalls),\n",
        "        'std': np.std(recalls),\n",
        "        'min': np.min(recalls),\n",
        "        'max': np.max(recalls),\n",
        "        'p50': np.percentile(recalls, 50),\n",
        "        'p95': np.percentile(recalls, 95)\n",
        "    }\n",
        "    \n",
        "    search_time_results[method_name] = {\n",
        "        'mean': np.mean(search_times),\n",
        "        'std': np.std(search_times),\n",
        "        'min': np.min(search_times),\n",
        "        'max': np.max(search_times)\n",
        "    }\n",
        "    \n",
        "    print(f\"  平均recall@100: {recall_results[method_name]['mean']:.3f}\")\n",
        "    print(f\"  平均搜索时间: {search_time_results[method_name]['mean']:.2f}ms\")\n",
        "\n",
        "# 显示recall结果汇总\n",
        "print(f\"\\n=== Recall@100结果汇总 ===\")\n",
        "print(f\"{'方法':<20} {'平均Recall':<12} {'标准差':<10} {'P50':<8} {'P95':<8}\")\n",
        "print(\"-\" * 70)\n",
        "for method_name, results in recall_results.items():\n",
        "    print(f\"{method_name:<20} {results['mean']:<12.3f} {results['std']:<10.3f} {results['p50']:<8.3f} {results['p95']:<8.3f}\")\n",
        "\n",
        "# 显示搜索时间汇总\n",
        "print(f\"\\n=== 搜索时间结果汇总 (ms) ===\")\n",
        "print(f\"{'方法':<20} {'平均时间':<12} {'标准差':<10} {'最小值':<10} {'最大值':<10}\")\n",
        "print(\"-\" * 80)\n",
        "for method_name, results in search_time_results.items():\n",
        "    print(f\"{method_name:<20} {results['mean']:<12.2f} {results['std']:<10.2f} {results['min']:<10.2f} {results['max']:<10.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 步骤4: 测试不同Recall目标下的搜索步长 ===\n",
        "print(\"\\n=== 步骤4: 测试不同Recall目标下的搜索步长 ===\")\n",
        "\n",
        "def find_ef_for_recall_target(index, queries, ground_truths, target_recall, k=100, ef_range=(32, 512)):\n",
        "    \"\"\"\n",
        "    找到达到目标recall所需的最小ef_search值\n",
        "    \n",
        "    Args:\n",
        "        index: HNSW索引\n",
        "        queries: 查询向量列表\n",
        "        ground_truths: 对应的ground truth列表\n",
        "        target_recall: 目标recall值 (如0.90, 0.95)\n",
        "        k: top-k值\n",
        "        ef_range: ef_search搜索范围 (min, max)\n",
        "    \n",
        "    Returns:\n",
        "        (optimal_ef, achieved_recall, search_steps): 最优ef值、达到的recall、搜索步数\n",
        "    \"\"\"\n",
        "    ef_min, ef_max = ef_range\n",
        "    \n",
        "    # 测试不同的ef值\n",
        "    test_efs = [ef_min, ef_min + (ef_max - ef_min) // 4, ef_min + (ef_max - ef_min) // 2, \n",
        "                ef_min + 3 * (ef_max - ef_min) // 4, ef_max]\n",
        "    \n",
        "    best_ef = ef_max\n",
        "    best_recall = 0.0\n",
        "    \n",
        "    for ef in test_efs:\n",
        "        recalls = []\n",
        "        search_steps = []\n",
        "        \n",
        "        for query, gt in zip(queries, ground_truths):\n",
        "            results, steps = index.query_with_steps(query, k=k, ef=ef)\n",
        "            recall = calculate_recall_at_k(results, gt, k)\n",
        "            recalls.append(recall)\n",
        "            search_steps.append(steps)\n",
        "        \n",
        "        mean_recall = np.mean(recalls)\n",
        "        mean_steps = np.mean(search_steps)\n",
        "        \n",
        "        if mean_recall >= target_recall and ef < best_ef:\n",
        "            best_ef = ef\n",
        "            best_recall = mean_recall\n",
        "    \n",
        "    # 计算最优ef对应的搜索步数\n",
        "    final_recalls = []\n",
        "    final_steps = []\n",
        "    for query, gt in zip(queries, ground_truths):\n",
        "        results, steps = index.query_with_steps(query, k=k, ef=best_ef)\n",
        "        recall = calculate_recall_at_k(results, gt, k)\n",
        "        final_recalls.append(recall)\n",
        "        final_steps.append(steps)\n",
        "    \n",
        "    return best_ef, np.mean(final_recalls), np.mean(final_steps)\n",
        "\n",
        "# 测试不同recall目标（使用P10-P99的查询）\n",
        "recall_targets = [0.70, 0.80, 0.85, 0.90, 0.95]\n",
        "recall_step_results = {}\n",
        "\n",
        "print(\"测试不同recall目标下的搜索步长...\")\n",
        "\n",
        "for method_name, index in methods.items():\n",
        "    print(f\"\\n--- 测试 {method_name} 的recall步长关系 ---\")\n",
        "    \n",
        "    method_results = {}\n",
        "    \n",
        "    for target_recall in recall_targets:\n",
        "        print(f\"  测试目标recall={target_recall*100:.0f}%...\")\n",
        "        \n",
        "        # 使用P10-P99的查询进行测试\n",
        "        test_queries_for_recall = []\n",
        "        test_ground_truths_for_recall = []\n",
        "        \n",
        "        for p in percentiles:\n",
        "            query_id = query_ids_by_percentile[p]\n",
        "            if query_id < len(train_queries):\n",
        "                test_queries_for_recall.append(train_queries[query_id])\n",
        "                ground_truth_ids = faiss_ground_truth.get(str(query_id), [])\n",
        "                test_ground_truths_for_recall.append(ground_truth_ids)\n",
        "        \n",
        "        if len(test_queries_for_recall) > 0:\n",
        "            optimal_ef, achieved_recall, search_steps = find_ef_for_recall_target(\n",
        "                index, test_queries_for_recall, test_ground_truths_for_recall, target_recall, k=100\n",
        "            )\n",
        "            \n",
        "            method_results[target_recall] = {\n",
        "                'ef_search': optimal_ef,\n",
        "                'achieved_recall': achieved_recall,\n",
        "                'search_steps': search_steps\n",
        "            }\n",
        "            \n",
        "            print(f\"    目标recall: {target_recall*100:.0f}%\")\n",
        "            print(f\"    最优ef_search: {optimal_ef}\")\n",
        "            print(f\"    达到的recall: {achieved_recall:.3f}\")\n",
        "            print(f\"    平均搜索步数: {search_steps:.1f}\")\n",
        "    \n",
        "    recall_step_results[method_name] = method_results\n",
        "\n",
        "# 显示结果汇总\n",
        "print(f\"\\n=== 不同Recall目标下的搜索步长汇总 ===\")\n",
        "for method_name, method_results in recall_step_results.items():\n",
        "    print(f\"\\n--- {method_name} ---\")\n",
        "    print(f\"{'目标Recall':<12} {'最优ef':<10} {'达到Recall':<12} {'搜索步数':<10}\")\n",
        "    print(\"-\" * 50)\n",
        "    for target, results in method_results.items():\n",
        "        print(f\"{target*100:>8.0f}%{'':<4} {results['ef_search']:<10} {results['achieved_recall']:<12.3f} {results['search_steps']:<10.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 步骤5: 可视化结果和总结报告 ===\n",
        "print(\"\\n=== 步骤5: 可视化结果和总结报告 ===\")\n",
        "\n",
        "# 绘制对比图表\n",
        "plt.figure(figsize=(20, 12))\n",
        "\n",
        "# 子图1: Recall对比\n",
        "plt.subplot(2, 3, 1)\n",
        "method_names = list(recall_results.keys())\n",
        "mean_recalls = [recall_results[method]['mean'] for method in method_names]\n",
        "std_recalls = [recall_results[method]['std'] for method in method_names]\n",
        "\n",
        "bars = plt.bar(method_names, mean_recalls, yerr=std_recalls, capsize=5, \n",
        "               color=['skyblue', 'lightgreen', 'lightcoral'])\n",
        "plt.xlabel('方法')\n",
        "plt.ylabel('平均Recall@100')\n",
        "plt.title('三种方法的Recall性能对比')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 添加数值标签\n",
        "for bar, mean_val in zip(bars, mean_recalls):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
        "             f'{mean_val:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# 子图2: 搜索时间对比\n",
        "plt.subplot(2, 3, 2)\n",
        "mean_times = [search_time_results[method]['mean'] for method in method_names]\n",
        "std_times = [search_time_results[method]['std'] for method in method_names]\n",
        "\n",
        "bars = plt.bar(method_names, mean_times, yerr=std_times, capsize=5,\n",
        "               color=['orange', 'lightblue', 'lightpink'])\n",
        "plt.xlabel('方法')\n",
        "plt.ylabel('平均搜索时间 (ms)')\n",
        "plt.title('三种方法的搜索时间对比')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 添加数值标签\n",
        "for bar, mean_val in zip(bars, mean_times):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "             f'{mean_val:.1f}', ha='center', va='bottom')\n",
        "\n",
        "# 子图3: Recall vs 搜索步数关系\n",
        "plt.subplot(2, 3, 3)\n",
        "colors = ['blue', 'green', 'red']\n",
        "markers = ['o', 's', '^']\n",
        "\n",
        "for i, (method_name, method_results) in enumerate(recall_step_results.items()):\n",
        "    if method_results:\n",
        "        targets = list(method_results.keys())\n",
        "        steps = [method_results[target]['search_steps'] for target in targets]\n",
        "        recalls = [method_results[target]['achieved_recall'] for target in targets]\n",
        "        \n",
        "        plt.plot(steps, recalls, marker=markers[i], color=colors[i], \n",
        "                linewidth=2, markersize=8, label=method_name)\n",
        "\n",
        "plt.xlabel('搜索步数')\n",
        "plt.ylabel('Recall@100')\n",
        "plt.title('Recall vs 搜索步数关系')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 子图4: FAISS Baseline搜索时间分布\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.hist(faiss_search_efforts, bins=50, alpha=0.7, edgecolor='black', color='lightgray')\n",
        "plt.xlabel('搜索时间 (ms)')\n",
        "plt.ylabel('频次')\n",
        "plt.title('FAISS Baseline搜索时间分布')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 添加分位数线\n",
        "for p, value in effort_percentiles.items():\n",
        "    plt.axvline(value, color='red', linestyle='--', alpha=0.7, linewidth=1)\n",
        "\n",
        "# 子图5: 不同ef_search下的Recall对比\n",
        "plt.subplot(2, 3, 5)\n",
        "# 这里可以添加不同ef_search值的对比\n",
        "ef_values = [32, 64, 128, 256]\n",
        "recall_by_ef = {}\n",
        "\n",
        "for method_name, index in methods.items():\n",
        "    recall_by_ef[method_name] = []\n",
        "    for ef in ef_values:\n",
        "        recalls = []\n",
        "        for i, query in enumerate(test_sample_queries[:100]):  # 使用100个查询快速测试\n",
        "            gt_query_id = train_query_size + i\n",
        "            ground_truth_ids = faiss_ground_truth.get(str(gt_query_id % train_query_size), [])\n",
        "            results = index.query(query, k=100, ef=ef)\n",
        "            recall = calculate_recall_at_k(results, ground_truth_ids, 100)\n",
        "            recalls.append(recall)\n",
        "        recall_by_ef[method_name].append(np.mean(recalls))\n",
        "\n",
        "for i, (method_name, recalls) in enumerate(recall_by_ef.items()):\n",
        "    plt.plot(ef_values, recalls, marker=markers[i], color=colors[i], \n",
        "             linewidth=2, markersize=8, label=method_name)\n",
        "\n",
        "plt.xlabel('ef_search')\n",
        "plt.ylabel('平均Recall@100')\n",
        "plt.title('不同ef_search下的Recall对比')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 子图6: 索引构建时间对比\n",
        "plt.subplot(2, 3, 6)\n",
        "build_times = [status_build_time, high_build_time, norm_build_time]\n",
        "bars = plt.bar(method_names, build_times, color=['gold', 'silver', 'bronze'])\n",
        "plt.xlabel('方法')\n",
        "plt.ylabel('构建时间 (秒)')\n",
        "plt.title('索引构建时间对比')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 添加数值标签\n",
        "for bar, time_val in zip(bars, build_times):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
        "             f'{time_val:.1f}s', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 总结报告\n",
        "print(\"\\n=== 三种HNSW方法对比总结报告 ===\")\n",
        "print(f\"\\n1. 索引构建性能:\")\n",
        "for i, method_name in enumerate(method_names):\n",
        "    print(f\"   {method_name}: {build_times[i]:.2f}秒\")\n",
        "\n",
        "print(f\"\\n2. Recall@100性能 (ef_search=32):\")\n",
        "for method_name, results in recall_results.items():\n",
        "    print(f\"   {method_name}: {results['mean']:.3f} ± {results['std']:.3f}\")\n",
        "\n",
        "print(f\"\\n3. 搜索时间性能 (ef_search=32):\")\n",
        "for method_name, results in search_time_results.items():\n",
        "    print(f\"   {method_name}: {results['mean']:.2f}ms ± {results['std']:.2f}ms\")\n",
        "\n",
        "print(f\"\\n4. FAISS Baseline参考:\")\n",
        "print(f\"   平均搜索时间: {np.mean(faiss_search_efforts):.2f}ms\")\n",
        "print(f\"   搜索时间范围: {np.min(faiss_search_efforts):.2f}ms - {np.max(faiss_search_efforts):.2f}ms\")\n",
        "\n",
        "print(f\"\\n5. 性能建议:\")\n",
        "best_recall_method = max(recall_results.keys(), key=lambda x: recall_results[x]['mean'])\n",
        "fastest_method = min(search_time_results.keys(), key=lambda x: search_time_results[x]['mean'])\n",
        "print(f\"   最佳Recall: {best_recall_method} ({recall_results[best_recall_method]['mean']:.3f})\")\n",
        "print(f\"   最快搜索: {fastest_method} ({search_time_results[fastest_method]['mean']:.2f}ms)\")\n",
        "\n",
        "print(f\"\\n6. 下一步优化方向:\")\n",
        "print(f\"   - 可以调整ef_search参数平衡recall和速度\")\n",
        "print(f\"   - 可以优化索引构建参数提高质量\")\n",
        "print(f\"   - 可以实现真正的高维映射Norm方法\")\n",
        "print(f\"   - 可以优化高层新增边的逻辑\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "- 仅 layer-1 的聚类；可能需要更高层的处理 (threshold)\n",
        "- query辅助的问题 (从静态开始)\n",
        "- 分析 映射-recall 问题 (第一阶段/第二阶段)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "什么时候选择 多少个 入口节点？\n",
        "- Hardness定义"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "- RoarGraph 源码校验(用同样的数据集 1M的 + 100K query)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
