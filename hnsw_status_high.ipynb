{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "\n",
      "\n",
      "reading image vector: ---\n",
      "<class 'numpy.ndarray'>\n",
      "2 (1000000, 200) float32 200000000\n",
      "\n",
      "\n",
      "reading querys: ---\n",
      "<class 'numpy.ndarray'>\n",
      "2 (100000, 200) float32 20000000\n"
     ]
    }
   ],
   "source": [
    "# HNSW Status High - 高层新增边方法测试\n",
    "from io_utils import read_fbin, read_ibin\n",
    "import faiss\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(f\"FAISS版本: {faiss.__version__}\")\n",
    "\n",
    "# 数据路径\n",
    "file_path = \"/root/code/vectordbindexing/Text2Image/base.1M.fbin\"\n",
    "query_path = \"/root/code/vectordbindexing/Text2Image/query.public.100K.fbin\"\n",
    "ground_truth_path = \"/root/code/vectordbindexing/Text2Image/groundtruth.public.100K.ibin\"\n",
    "\n",
    "# 读取数据集\n",
    "print(\"\\n=== 读取数据集 ===\")\n",
    "print(\"读取图像向量...\")\n",
    "data_vector = read_fbin(file_path)\n",
    "print(f\"图像向量: {data_vector.shape}, dtype: {data_vector.dtype}\")\n",
    "\n",
    "print(\"读取查询向量...\")\n",
    "query_vector = read_fbin(query_path)\n",
    "print(f\"查询向量: {query_vector.shape}, dtype: {query_vector.dtype}\")\n",
    "\n",
    "# 使用前100K数据进行测试\n",
    "train_data_vector = data_vector[:100000]\n",
    "print(f\"训练数据: {train_data_vector.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnsw_cosine_status_high as hnsw_cosine\n",
    "import simple_sim_hash\n",
    "import importlib\n",
    "importlib.reload(hnsw_cosine)\n",
    "\n",
    "# M=64 比较合适，甚至更宽的宽度\n",
    "# 这里是个经验值：会在增加宽度的同时，逐渐达到一个稳定值\n",
    "index = hnsw_cosine.HNSWIndex(M=64, ef_construction=128, ef_search=64, random_seed=1)\n",
    "simHash = simple_sim_hash.SimpleSimHash(dim=200)\n",
    "\n",
    "IMAGE_IDX_SET = set()\n",
    "\n",
    "# 形状 [N,200]（先用1M子集或更小切片做原型）\n",
    "for img_id, vec in enumerate(train_data_vector):        # 可加 tqdm、批量 flush\n",
    "    index.add_item_fast10k(vec, lsh=simHash, limit=100)\n",
    "    IMAGE_IDX_SET.add(img_id)\n",
    "\n",
    "for qid, vec in enumerate(query_vector):\n",
    "    index.add_item_fast10k(vec, lsh=simHash, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train: 83333\n",
      "num of test: 16667\n"
     ]
    }
   ],
   "source": [
    "# 读取faiss搜索结果，获取 query_vector 和 search 结果\n",
    "import json\n",
    "train_query_list = {}\n",
    "test_query_list = {}\n",
    "\n",
    "with open(\"./TempResults/search_results_100K.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    for query_idx, vec_list in data.items():\n",
    "        mList = []\n",
    "        for x in vec_list:\n",
    "            mList.append(x - int(query_idx))\n",
    "        if int(query_idx) % 6 != 0:\n",
    "            train_query_list[int(query_idx)] = mList\n",
    "        else:\n",
    "            test_query_list[int(query_idx)] = mList\n",
    "print(f\"num of train: {len(train_query_list)}\")\n",
    "print(f\"num of test: {len(test_query_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 阶段分析统计 ===\n",
      "第一阶段 (快速靠近) - 平均步数: 406.25, 平均加速边: 0.00\n",
      "第二阶段 (Beam Search) - 平均步数: 72.71, 平均加速边: 0.00\n",
      "整体加速边使用比例: 0.00%\n",
      "\n",
      "加速边受益最多的查询:\n",
      "  第一阶段: 412 步, 0 条加速边\n",
      "  第二阶段: 110 步, 0 条加速边\n",
      "  总步数: 522, 总加速边: 0\n",
      "  加速边比例: 0.00%\n",
      "\n",
      "原始搜索统计:\n",
      "mean steps: 478.96458644371455\n",
      "middle steps: 460.0\n",
      "p99 steps: 906.0\n"
     ]
    }
   ],
   "source": [
    "# OOD search steps\n",
    "NUM_STEPS = []\n",
    "PHASE_ANALYSIS = []\n",
    "for qid, target_list in test_query_list.items():\n",
    "    q = query_vector[qid]\n",
    "    for target_id in target_list[:10]:\n",
    "        if target_id not in IMAGE_IDX_SET:\n",
    "            continue\n",
    "        # 使用阶段分析功能\n",
    "        out = index.search_steps_to_target(q, target_id, k=10, ef=64, analyze_phases=True, verbose=False, multi_path_search=True, max_paths=3)\n",
    "        NUM_STEPS.append(len(out[\"trace\"]))\n",
    "        if \"phase_analysis\" in out:\n",
    "            PHASE_ANALYSIS.append(out[\"phase_analysis\"])\n",
    "\n",
    "\n",
    "# 分析阶段统计\n",
    "if PHASE_ANALYSIS:\n",
    "    print(\"\\n=== 阶段分析统计 ===\")\n",
    "    phase_1_steps = [pa[\"phase_1\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_steps = [pa[\"phase_2\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_1_accel_edges = [pa[\"phase_1\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_accel_edges = [pa[\"phase_2\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    \n",
    "    print(f\"第一阶段 (快速靠近) - 平均步数: {np.mean(phase_1_steps):.2f}, 平均加速边: {np.mean(phase_1_accel_edges):.2f}\")\n",
    "    print(f\"第二阶段 (Beam Search) - 平均步数: {np.mean(phase_2_steps):.2f}, 平均加速边: {np.mean(phase_2_accel_edges):.2f}\")\n",
    "    \n",
    "    # 计算加速边使用比例\n",
    "    total_accel_edges = [pa[\"total_accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    total_steps = [pa[\"total_steps\"] for pa in PHASE_ANALYSIS]\n",
    "    accel_edge_ratios = [accel/steps if steps > 0 else 0 for accel, steps in zip(total_accel_edges, total_steps)]\n",
    "    \n",
    "    print(f\"整体加速边使用比例: {np.mean(accel_edge_ratios):.2%}\")\n",
    "    \n",
    "    # 分析哪些查询受益最多\n",
    "    if len(PHASE_ANALYSIS) > 0:\n",
    "        best_benefit_idx = np.argmax(accel_edge_ratios)\n",
    "        best_benefit = PHASE_ANALYSIS[best_benefit_idx]\n",
    "        print(f\"\\n加速边受益最多的查询:\")\n",
    "        print(f\"  第一阶段: {best_benefit['phase_1']['step_count']} 步, {best_benefit['phase_1']['accel_edges']} 条加速边\")\n",
    "        print(f\"  第二阶段: {best_benefit['phase_2']['step_count']} 步, {best_benefit['phase_2']['accel_edges']} 条加速边\")\n",
    "        print(f\"  总步数: {best_benefit['total_steps']}, 总加速边: {best_benefit['total_accel_edges']}\")\n",
    "        print(f\"  加速边比例: {best_benefit['overall_accel_edge_ratio']:.2%}\")\n",
    "\n",
    "\n",
    "arr_ori_bak = np.array(NUM_STEPS, dtype=np.float64)\n",
    "arr_ori = arr_ori_bak.copy()\n",
    "arr_ori.sort()\n",
    "\n",
    "mean_steps = arr_ori.mean()\n",
    "P50_steps = np.percentile(arr_ori, 50)\n",
    "p99_steps = np.percentile(arr_ori, 99)\n",
    "print(f\"\\n原始搜索统计:\")\n",
    "print(f\"mean steps: {mean_steps}\")\n",
    "print(f\"middle steps: {P50_steps}\")\n",
    "print(f\"p99 steps: {p99_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 构建 RoarGraph 风格的 Cross Distribution 边 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross distribution 边构建统计:\n",
      "{'query_processed': True, 'layer_1_nodes_total': 3092, 'top_k_selected': 10, 'pairs_considered': 45, 'pairs_added': 10, 'skipped_existing': 35, 'pruned_by_cap': 0, 'edges_added': 10, 'top_k_nodes': [123656, 125677, 158436, 183136, 132312, 179175, 141094, 190128, 156261, 174419], 'query_distance': 0.3494441509246826}\n",
      "\n",
      "Cross distribution 边统计:\n",
      "总添加的 cross distribution 边: 1512998\n",
      "被删除的 cross distribution 边: 1374116\n",
      "活跃的 cross distribution 边: 138882\n"
     ]
    }
   ],
   "source": [
    "# 使用新的 RoarGraph 风格的 cross distribution 边构建\n",
    "print(\"\\n=== 构建 RoarGraph 风格的 Cross Distribution 边 ===\")\n",
    "for query in query_vector:\n",
    "    stats = index.build_cross_distribution_edges(\n",
    "        max_new_edges_per_node=4,\n",
    "        query=query,\n",
    "    )\n",
    "print(\"Cross distribution 边构建统计:\")\n",
    "print(stats)\n",
    "\n",
    "# 获取 cross distribution 边的统计信息\n",
    "cross_stats = index.get_cross_distribution_stats()\n",
    "print(\"\\nCross distribution 边统计:\")\n",
    "print(f\"总添加的 cross distribution 边: {cross_stats['total_cross_edges']}\")\n",
    "print(f\"被删除的 cross distribution 边: {cross_stats['deleted_cross_edges']}\")\n",
    "print(f\"活跃的 cross distribution 边: {cross_stats['active_cross_edges']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 阶段分析统计 ===\n",
      "第一阶段 (快速靠近) - 平均步数: 525.21, 平均加速边: 275.88\n",
      "第二阶段 (Beam Search) - 平均步数: 72.71, 平均加速边: 0.00\n",
      "整体加速边使用比例: 47.27%\n",
      "\n",
      "加速边受益最多的查询:\n",
      "  第一阶段: 372 步, 267 条加速边\n",
      "  第二阶段: 0 步, 0 条加速边\n",
      "  总步数: 372, 总加速边: 267\n",
      "  加速边比例: 71.77%\n",
      "\n",
      "原始搜索统计:\n",
      "mean steps: 597.9242759032547\n",
      "middle steps: 585.0\n",
      "p99 steps: 1040.0\n"
     ]
    }
   ],
   "source": [
    "# OOD search steps\n",
    "NUM_STEPS = []\n",
    "PHASE_ANALYSIS = []\n",
    "for qid, target_list in test_query_list.items():\n",
    "    q = query_vector[qid]\n",
    "    for target_id in target_list[:10]:\n",
    "        if target_id not in IMAGE_IDX_SET:\n",
    "            continue\n",
    "        # 使用阶段分析功能\n",
    "        out = index.search_steps_to_target(q, target_id, k=10, ef=64, analyze_phases=True, verbose=False, multi_path_search=True, max_paths=3)\n",
    "        NUM_STEPS.append(len(out[\"trace\"]))\n",
    "        if \"phase_analysis\" in out:\n",
    "            PHASE_ANALYSIS.append(out[\"phase_analysis\"])\n",
    "\n",
    "\n",
    "# 分析阶段统计\n",
    "if PHASE_ANALYSIS:\n",
    "    print(\"\\n=== 阶段分析统计 ===\")\n",
    "    phase_1_steps = [pa[\"phase_1\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_steps = [pa[\"phase_2\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_1_accel_edges = [pa[\"phase_1\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_accel_edges = [pa[\"phase_2\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    \n",
    "    print(f\"第一阶段 (快速靠近) - 平均步数: {np.mean(phase_1_steps):.2f}, 平均加速边: {np.mean(phase_1_accel_edges):.2f}\")\n",
    "    print(f\"第二阶段 (Beam Search) - 平均步数: {np.mean(phase_2_steps):.2f}, 平均加速边: {np.mean(phase_2_accel_edges):.2f}\")\n",
    "    \n",
    "    # 计算加速边使用比例\n",
    "    total_accel_edges = [pa[\"total_accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    total_steps = [pa[\"total_steps\"] for pa in PHASE_ANALYSIS]\n",
    "    accel_edge_ratios = [accel/steps if steps > 0 else 0 for accel, steps in zip(total_accel_edges, total_steps)]\n",
    "    \n",
    "    print(f\"整体加速边使用比例: {np.mean(accel_edge_ratios):.2%}\")\n",
    "    \n",
    "    # 分析哪些查询受益最多\n",
    "    if len(PHASE_ANALYSIS) > 0:\n",
    "        best_benefit_idx = np.argmax(accel_edge_ratios)\n",
    "        best_benefit = PHASE_ANALYSIS[best_benefit_idx]\n",
    "        print(f\"\\n加速边受益最多的查询:\")\n",
    "        print(f\"  第一阶段: {best_benefit['phase_1']['step_count']} 步, {best_benefit['phase_1']['accel_edges']} 条加速边\")\n",
    "        print(f\"  第二阶段: {best_benefit['phase_2']['step_count']} 步, {best_benefit['phase_2']['accel_edges']} 条加速边\")\n",
    "        print(f\"  总步数: {best_benefit['total_steps']}, 总加速边: {best_benefit['total_accel_edges']}\")\n",
    "        print(f\"  加速边比例: {best_benefit['overall_accel_edge_ratio']:.2%}\")\n",
    "\n",
    "\n",
    "arr_ori_bak = np.array(NUM_STEPS, dtype=np.float64)\n",
    "arr_ori = arr_ori_bak.copy()\n",
    "arr_ori.sort()\n",
    "\n",
    "mean_steps = arr_ori.mean()\n",
    "P50_steps = np.percentile(arr_ori, 50)\n",
    "p99_steps = np.percentile(arr_ori, 99)\n",
    "print(f\"\\n原始搜索统计:\")\n",
    "print(f\"mean steps: {mean_steps}\")\n",
    "print(f\"middle steps: {P50_steps}\")\n",
    "print(f\"p99 steps: {p99_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新增：真实Recall测试功能\n",
    "print(\"\\n=== 新增：真实Recall@100测试 ===\")\n",
    "\n",
    "# 读取ground truth数据\n",
    "print(\"读取ground truth数据...\")\n",
    "ground_truth = read_ibin(ground_truth_path)\n",
    "print(f\"Ground truth形状: {ground_truth.shape}\")\n",
    "\n",
    "def calculate_recall_at_k(predicted_ids, ground_truth_ids, k):\n",
    "    \"\"\"\n",
    "    计算recall@k\n",
    "    \n",
    "    Args:\n",
    "        predicted_ids: 预测的top-k结果\n",
    "        ground_truth_ids: ground truth结果\n",
    "        k: top-k值\n",
    "    \n",
    "    Returns:\n",
    "        recall@k值\n",
    "    \"\"\"\n",
    "    # 取前k个预测结果\n",
    "    top_k_pred = set(predicted_ids[:k])\n",
    "    \n",
    "    # 取ground truth中在索引范围内的结果\n",
    "    valid_gt = set()\n",
    "    for gt_id in ground_truth_ids:\n",
    "        if gt_id in IMAGE_IDX_SET:  # 确保ground truth在索引中\n",
    "            valid_gt.add(gt_id)\n",
    "    \n",
    "    if len(valid_gt) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 计算交集\n",
    "    intersection = top_k_pred.intersection(valid_gt)\n",
    "    \n",
    "    # recall@k = |intersection| / |ground_truth|\n",
    "    recall = len(intersection) / len(valid_gt)\n",
    "    return recall\n",
    "\n",
    "# 验证ground truth数据格式\n",
    "print(f\"\\n=== Ground Truth数据格式分析 ===\")\n",
    "print(f\"前5个查询的ground truth:\")\n",
    "for i in range(min(5, len(ground_truth))):\n",
    "    print(f\"  查询{i}: {ground_truth[i][:10]}... (共{len(ground_truth[i])}个)\")\n",
    "\n",
    "# 验证ground truth索引是否在范围内\n",
    "gt_min, gt_max = ground_truth.min(), ground_truth.max()\n",
    "print(f\"\\nGround truth索引范围: {gt_min} - {gt_max}\")\n",
    "print(f\"训练数据索引范围: 0 - {len(IMAGE_IDX_SET)-1}\")\n",
    "valid_gt_count = np.sum(np.isin(ground_truth, list(IMAGE_IDX_SET)))\n",
    "total_gt_count = ground_truth.size\n",
    "print(f\"Ground truth中有多少在训练数据范围内: {valid_gt_count} / {total_gt_count} ({valid_gt_count/total_gt_count*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试真实的recall - 返回top100并与ground truth对比\n",
    "print(\"\\n=== 测试1: 真实Recall@100测试 ===\")\n",
    "\n",
    "# 选择测试查询（使用前500个查询进行测试）\n",
    "test_query_count = 500\n",
    "test_queries = query_vector[:test_query_count]\n",
    "test_ground_truth = ground_truth[:test_query_count]\n",
    "\n",
    "# 不同ef_search值的测试\n",
    "ef_values = [32, 64, 128, 256]\n",
    "recall_results = {}\n",
    "search_time_results = {}\n",
    "search_steps_results = {}\n",
    "\n",
    "for ef in ef_values:\n",
    "    print(f\"\\n测试 ef_search={ef}...\")\n",
    "    \n",
    "    recalls = []\n",
    "    search_times = []\n",
    "    search_steps_list = []\n",
    "    \n",
    "    for i, (query, gt) in enumerate(zip(test_queries, test_ground_truth)):\n",
    "        # 搜索\n",
    "        start_time = time.time()\n",
    "        results, search_steps = index.query_with_steps(query, k=100, ef=ef)\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        # 计算recall@100\n",
    "        recall = calculate_recall_at_k(results, gt, 100)\n",
    "        \n",
    "        recalls.append(recall)\n",
    "        search_times.append(search_time)\n",
    "        search_steps_list.append(search_steps)\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  完成 {i+1}/{test_query_count} 个查询\")\n",
    "    \n",
    "    # 统计结果\n",
    "    recall_results[ef] = {\n",
    "        'mean': np.mean(recalls),\n",
    "        'std': np.std(recalls),\n",
    "        'min': np.min(recalls),\n",
    "        'max': np.max(recalls),\n",
    "        'p50': np.percentile(recalls, 50),\n",
    "        'p95': np.percentile(recalls, 95),\n",
    "        'p99': np.percentile(recalls, 99)\n",
    "    }\n",
    "    \n",
    "    search_time_results[ef] = {\n",
    "        'mean': np.mean(search_times),\n",
    "        'std': np.std(search_times),\n",
    "        'min': np.min(search_times),\n",
    "        'max': np.max(search_times)\n",
    "    }\n",
    "    \n",
    "    search_steps_results[ef] = {\n",
    "        'mean': np.mean(search_steps_list),\n",
    "        'std': np.std(search_steps_list),\n",
    "        'min': np.min(search_steps_list),\n",
    "        'max': np.max(search_steps_list)\n",
    "    }\n",
    "    \n",
    "    print(f\"  ef_search={ef}: 平均recall@100={recall_results[ef]['mean']:.3f}\")\n",
    "    print(f\"    平均搜索时间={search_time_results[ef]['mean']:.4f}s\")\n",
    "    print(f\"    平均搜索步数={search_steps_results[ef]['mean']:.1f}\")\n",
    "\n",
    "# 显示recall结果汇总\n",
    "print(f\"\\n=== Recall@100结果汇总 ===\")\n",
    "print(f\"{'ef_search':<10} {'mean':<8} {'std':<8} {'p50':<8} {'p95':<8} {'p99':<8}\")\n",
    "print(\"-\" * 60)\n",
    "for ef in ef_values:\n",
    "    r = recall_results[ef]\n",
    "    print(f\"{ef:<10} {r['mean']:<8.3f} {r['std']:<8.3f} {r['p50']:<8.3f} {r['p95']:<8.3f} {r['p99']:<8.3f}\")\n",
    "\n",
    "# 显示搜索时间汇总\n",
    "print(f\"\\n=== 搜索时间结果汇总 (秒) ===\")\n",
    "print(f\"{'ef_search':<10} {'mean':<10} {'std':<10} {'min':<10} {'max':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for ef in ef_values:\n",
    "    t = search_time_results[ef]\n",
    "    print(f\"{ef:<10} {t['mean']:<10.4f} {t['std']:<10.4f} {t['min']:<10.4f} {t['max']:<10.4f}\")\n",
    "\n",
    "# 显示搜索步数汇总\n",
    "print(f\"\\n=== 搜索步数结果汇总 ===\")\n",
    "print(f\"{'ef_search':<10} {'mean':<10} {'std':<10} {'min':<10} {'max':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for ef in ef_values:\n",
    "    s = search_steps_results[ef]\n",
    "    print(f\"{ef:<10} {s['mean']:<10.1f} {s['std']:<10.1f} {s['min']:<10.1f} {s['max']:<10.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试不同recall概率下的搜索步长\n",
    "print(\"\\n=== 测试2: 不同Recall概率下的搜索步长测试 ===\")\n",
    "\n",
    "def find_ef_for_recall_target(index, queries, ground_truths, target_recall, k=100, ef_range=(32, 512)):\n",
    "    \"\"\"\n",
    "    找到达到目标recall所需的最小ef_search值\n",
    "    \n",
    "    Args:\n",
    "        index: HNSW索引\n",
    "        queries: 查询向量列表\n",
    "        ground_truths: 对应的ground truth列表\n",
    "        target_recall: 目标recall值 (如0.90, 0.95)\n",
    "        k: top-k值\n",
    "        ef_range: ef_search搜索范围 (min, max)\n",
    "    \n",
    "    Returns:\n",
    "        (optimal_ef, achieved_recall, search_steps): 最优ef值、达到的recall、搜索步数\n",
    "    \"\"\"\n",
    "    ef_min, ef_max = ef_range\n",
    "    \n",
    "    # 测试不同的ef值\n",
    "    test_efs = [ef_min, ef_min + (ef_max - ef_min) // 4, ef_min + (ef_max - ef_min) // 2, \n",
    "                ef_min + 3 * (ef_max - ef_min) // 4, ef_max]\n",
    "    \n",
    "    best_ef = ef_max\n",
    "    best_recall = 0.0\n",
    "    \n",
    "    for ef in test_efs:\n",
    "        recalls = []\n",
    "        search_steps = []\n",
    "        \n",
    "        for query, gt in zip(queries, ground_truths):\n",
    "            results, steps = index.query_with_steps(query, k=k, ef=ef)\n",
    "            recall = calculate_recall_at_k(results, gt, k)\n",
    "            recalls.append(recall)\n",
    "            search_steps.append(steps)\n",
    "        \n",
    "        mean_recall = np.mean(recalls)\n",
    "        mean_steps = np.mean(search_steps)\n",
    "        \n",
    "        if mean_recall >= target_recall and ef < best_ef:\n",
    "            best_ef = ef\n",
    "            best_recall = mean_recall\n",
    "    \n",
    "    # 如果最小值就能达到目标，测试更小的值\n",
    "    if mean_recall >= target_recall and ef == ef_min:\n",
    "        for ef in [16, 24, 32]:\n",
    "            recalls = []\n",
    "            search_steps = []\n",
    "            \n",
    "            for query, gt in zip(queries, ground_truths):\n",
    "                results, steps = index.query_with_steps(query, k=k, ef=ef)\n",
    "                recall = calculate_recall_at_k(results, gt, k)\n",
    "                recalls.append(recall)\n",
    "                search_steps.append(steps)\n",
    "            \n",
    "            mean_recall = np.mean(recalls)\n",
    "            mean_steps = np.mean(search_steps)\n",
    "            \n",
    "            if mean_recall >= target_recall and ef < best_ef:\n",
    "                best_ef = ef\n",
    "                best_recall = mean_recall\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    # 计算最优ef对应的搜索步数\n",
    "    final_recalls = []\n",
    "    final_steps = []\n",
    "    for query, gt in zip(queries, ground_truths):\n",
    "        results, steps = index.query_with_steps(query, k=k, ef=best_ef)\n",
    "        recall = calculate_recall_at_k(results, gt, k)\n",
    "        final_recalls.append(recall)\n",
    "        final_steps.append(steps)\n",
    "    \n",
    "    return best_ef, np.mean(final_recalls), np.mean(final_steps)\n",
    "\n",
    "# 测试不同recall目标（使用更小的查询集）\n",
    "recall_targets = [0.70, 0.80, 0.85, 0.90]\n",
    "recall_step_results = {}\n",
    "\n",
    "# 使用100个查询进行快速测试\n",
    "test_query_small = test_queries[:100]\n",
    "test_gt_small = test_ground_truth[:100]\n",
    "\n",
    "print(\"测试不同recall目标下的搜索步长...\")\n",
    "for target_recall in recall_targets:\n",
    "    print(f\"\\n测试目标recall={target_recall*100:.0f}%...\")\n",
    "    \n",
    "    optimal_ef, achieved_recall, search_steps = find_ef_for_recall_target(\n",
    "        index, test_query_small, test_gt_small, target_recall, k=100\n",
    "    )\n",
    "    \n",
    "    recall_step_results[target_recall] = {\n",
    "        'ef_search': optimal_ef,\n",
    "        'achieved_recall': achieved_recall,\n",
    "        'search_steps': search_steps\n",
    "    }\n",
    "    \n",
    "    print(f\"  目标recall: {target_recall*100:.0f}%\")\n",
    "    print(f\"  最优ef_search: {optimal_ef}\")\n",
    "    print(f\"  达到的recall: {achieved_recall:.3f}\")\n",
    "    print(f\"  平均搜索步数: {search_steps:.1f}\")\n",
    "\n",
    "# 显示结果汇总\n",
    "print(f\"\\n=== 不同Recall目标下的搜索步长汇总 ===\")\n",
    "print(f\"{'目标Recall':<12} {'最优ef':<10} {'达到Recall':<12} {'搜索步数':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for target, results in recall_step_results.items():\n",
    "    print(f\"{target*100:>8.0f}%{'':<4} {results['ef_search']:<10} {results['achieved_recall']:<12.3f} {results['search_steps']:<10.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化结果和总结报告\n",
    "print(\"\\n=== 测试3: 结果可视化 ===\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 绘制recall vs ef_search曲线\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 子图1: Recall@100 vs ef_search\n",
    "plt.subplot(1, 3, 1)\n",
    "ef_list = list(recall_results.keys())\n",
    "mean_recalls = [recall_results[ef]['mean'] for ef in ef_list]\n",
    "std_recalls = [recall_results[ef]['std'] for ef in ef_list]\n",
    "\n",
    "plt.errorbar(ef_list, mean_recalls, yerr=std_recalls, marker='o', capsize=5)\n",
    "plt.xlabel('ef_search')\n",
    "plt.ylabel('Recall@100')\n",
    "plt.title('Recall@100 vs ef_search')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 子图2: 搜索时间 vs ef_search\n",
    "plt.subplot(1, 3, 2)\n",
    "mean_times = [search_time_results[ef]['mean'] for ef in ef_list]\n",
    "std_times = [search_time_results[ef]['std'] for ef in ef_list]\n",
    "\n",
    "plt.errorbar(ef_list, mean_times, yerr=std_times, marker='s', capsize=5, color='orange')\n",
    "plt.xlabel('ef_search')\n",
    "plt.ylabel('搜索时间 (秒)')\n",
    "plt.title('搜索时间 vs ef_search')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 子图3: Recall目标 vs 搜索步数\n",
    "plt.subplot(1, 3, 3)\n",
    "target_list = list(recall_step_results.keys())\n",
    "step_list = [recall_step_results[target]['search_steps'] for target in target_list]\n",
    "ef_list_target = [recall_step_results[target]['ef_search'] for target in target_list]\n",
    "\n",
    "plt.plot([t*100 for t in target_list], step_list, marker='^', linewidth=2, markersize=8)\n",
    "plt.xlabel('目标Recall (%)')\n",
    "plt.ylabel('搜索步数')\n",
    "plt.title('不同Recall目标下的搜索步数')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 在第三个图上添加ef_search值作为标注\n",
    "for i, (target, steps, ef) in enumerate(zip(target_list, step_list, ef_list_target)):\n",
    "    plt.annotate(f'ef={ef}', (target*100, steps), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 总结报告\n",
    "print(\"\\n=== Recall测试总结报告 ===\")\n",
    "print(f\"1. 真实Recall@100测试:\")\n",
    "print(f\"   - 测试了 {test_query_count} 个查询\")\n",
    "print(f\"   - ef_search范围: {ef_values}\")\n",
    "print(f\"   - 最佳recall: {max(mean_recalls):.3f} (ef_search={ef_list[np.argmax(mean_recalls)]})\")\n",
    "print(f\"   - 最快搜索: {min(mean_times):.4f}s (ef_search={ef_list[np.argmin(mean_times)]})\")\n",
    "\n",
    "print(f\"\\n2. 不同Recall目标下的搜索步长:\")\n",
    "print(f\"   - 测试了 {len(recall_targets)} 个recall目标\")\n",
    "print(f\"   - 最高目标recall: {max(recall_step_results.keys())*100:.0f}%\")\n",
    "if recall_step_results:\n",
    "    print(f\"   - 最大搜索步数: {max(step_list):.1f} (recall={max(recall_step_results.keys())*100:.0f}%)\")\n",
    "    print(f\"   - 最小搜索步数: {min(step_list):.1f} (recall={min(recall_step_results.keys())*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\n3. 性能建议:\")\n",
    "best_recall_ef = ef_list[np.argmax(mean_recalls)]\n",
    "best_time_ef = ef_list[np.argmin(mean_times)]\n",
    "print(f\"   - 追求最高recall: 使用ef_search={best_recall_ef}\")\n",
    "print(f\"   - 追求最快速度: 使用ef_search={best_time_ef}\")\n",
    "print(f\"   - 平衡选择: ef_search=64-128 之间\")\n",
    "\n",
    "print(f\"\\n4. 数据质量分析:\")\n",
    "print(f\"   - Ground truth覆盖率: {valid_gt_count}/{total_gt_count} ({valid_gt_count/total_gt_count*100:.1f}%)\")\n",
    "print(f\"   - 这解释了为什么recall值可能较低\")\n",
    "\n",
    "print(f\"\\n5. 与原有搜索步数测试的对比:\")\n",
    "print(f\"   - 原有测试主要关注搜索到特定目标的步数\")\n",
    "print(f\"   - 新增测试关注整体recall性能和搜索效率\")\n",
    "print(f\"   - 两者结合可以全面评估HNSW索引的性能\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
