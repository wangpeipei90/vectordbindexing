{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "\n",
      "\n",
      "reading image vector: ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "2 (1000000, 200) float32 200000000\n",
      "\n",
      "\n",
      "reading querys: ---\n",
      "<class 'numpy.ndarray'>\n",
      "2 (100000, 200) float32 20000000\n"
     ]
    }
   ],
   "source": [
    "from io_utils import read_fbin, read_ibin\n",
    "import faiss\n",
    "print(faiss.__version__)\n",
    "import numpy as np\n",
    "file_path = \"/root/code/vectordbindexing/Text2Image/base.1M.fbin\"\n",
    "query_path = \"/root/code/vectordbindexing/Text2Image/query.public.100K.fbin\"\n",
    "query_train_path = \"/root/code/vectordbindexing/data/t2i-10M/query.train.10M.fbin\"\n",
    "ground_truth_path = \"/root/code/vectordbindexing/Text2Image/groundtruth.public.100K.ibin\"\n",
    "\n",
    "# read datasets\n",
    "print(\"\\n\\nreading image vector: ---\")\n",
    "data_vector = read_fbin(file_path)\n",
    "print(type(data_vector))\n",
    "print(data_vector.ndim, data_vector.shape, data_vector.dtype, data_vector.size)\n",
    "# print(data_vector[:1])  # Print first 1 elements to verify content\n",
    "\n",
    "train_data_vector = data_vector[:100000]\n",
    "insert_1_percent = data_vector[500000:505000]\n",
    "insert_2_percent = data_vector[505000:510000]\n",
    "insert_3_percent = data_vector[510000:515000]\n",
    "insert_4_percent = data_vector[515000:520000]\n",
    "insert_5_percent = data_vector[520000:525000]\n",
    "insert_10_percent = data_vector[525000:550000]\n",
    "\n",
    "# read querys\n",
    "print(\"\\n\\nreading querys: ---\")\n",
    "query_vector = read_fbin(query_path)\n",
    "print(type(query_vector))\n",
    "print(query_vector.ndim, query_vector.shape, query_vector.dtype, query_vector.size)\n",
    "# print(query_vector[0])  # Print first 3 elements to verify content\n",
    "\n",
    "# read train querys\n",
    "# print(\"\\n\\nreading train querys: ---\")\n",
    "# query_train_vector = read_fbin(query_train_path)\n",
    "# print(type(query_train_vector))\n",
    "# print(query_train_vector.ndim, query_train_vector.shape, query_train_vector.dtype, query_train_vector.size)\n",
    "# print(query_train_vector[0])  # Print first 3 elements to verify content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnsw_cosine_status_high as hnsw_cosine\n",
    "import simple_sim_hash\n",
    "import importlib\n",
    "importlib.reload(hnsw_cosine)\n",
    "\n",
    "# M=64 比较合适，甚至更宽的宽度\n",
    "# 这里是个经验值：会在增加宽度的同时，逐渐达到一个稳定值\n",
    "index = hnsw_cosine.HNSWIndex(M=64, ef_construction=128, ef_search=64, random_seed=1)\n",
    "simHash = simple_sim_hash.SimpleSimHash(dim=200)\n",
    "\n",
    "IMAGE_IDX_SET = set()\n",
    "\n",
    "# 形状 [N,200]（先用1M子集或更小切片做原型）\n",
    "for img_id, vec in enumerate(train_data_vector):        # 可加 tqdm、批量 flush\n",
    "    index.add_item_fast10k(vec, lsh=simHash, limit=100)\n",
    "    IMAGE_IDX_SET.add(img_id)\n",
    "\n",
    "for qid, vec in enumerate(query_vector):\n",
    "    index.add_item_fast10k(vec, lsh=simHash, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train: 83333\n",
      "num of test: 16667\n"
     ]
    }
   ],
   "source": [
    "# 读取faiss搜索结果，获取 query_vector 和 search 结果\n",
    "import json\n",
    "train_query_list = {}\n",
    "test_query_list = {}\n",
    "\n",
    "with open(\"./TempResults/search_results_100K.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    for query_idx, vec_list in data.items():\n",
    "        mList = []\n",
    "        for x in vec_list:\n",
    "            mList.append(x - int(query_idx))\n",
    "        if int(query_idx) % 6 != 0:\n",
    "            train_query_list[int(query_idx)] = mList\n",
    "        else:\n",
    "            test_query_list[int(query_idx)] = mList\n",
    "print(f\"num of train: {len(train_query_list)}\")\n",
    "print(f\"num of test: {len(test_query_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 阶段分析统计 ===\n",
      "第一阶段 (快速靠近) - 平均步数: 406.25, 平均加速边: 0.00\n",
      "第二阶段 (Beam Search) - 平均步数: 72.71, 平均加速边: 0.00\n",
      "整体加速边使用比例: 0.00%\n",
      "\n",
      "加速边受益最多的查询:\n",
      "  第一阶段: 412 步, 0 条加速边\n",
      "  第二阶段: 110 步, 0 条加速边\n",
      "  总步数: 522, 总加速边: 0\n",
      "  加速边比例: 0.00%\n",
      "\n",
      "原始搜索统计:\n",
      "mean steps: 478.96458644371455\n",
      "middle steps: 460.0\n",
      "p99 steps: 906.0\n"
     ]
    }
   ],
   "source": [
    "# OOD search steps\n",
    "NUM_STEPS = []\n",
    "PHASE_ANALYSIS = []\n",
    "for qid, target_list in test_query_list.items():\n",
    "    q = query_vector[qid]\n",
    "    for target_id in target_list[:10]:\n",
    "        if target_id not in IMAGE_IDX_SET:\n",
    "            continue\n",
    "        # 使用阶段分析功能\n",
    "        out = index.search_steps_to_target(q, target_id, k=10, ef=64, analyze_phases=True, verbose=False, multi_path_search=True, max_paths=3)\n",
    "        NUM_STEPS.append(len(out[\"trace\"]))\n",
    "        if \"phase_analysis\" in out:\n",
    "            PHASE_ANALYSIS.append(out[\"phase_analysis\"])\n",
    "\n",
    "\n",
    "# 分析阶段统计\n",
    "if PHASE_ANALYSIS:\n",
    "    print(\"\\n=== 阶段分析统计 ===\")\n",
    "    phase_1_steps = [pa[\"phase_1\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_steps = [pa[\"phase_2\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_1_accel_edges = [pa[\"phase_1\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_accel_edges = [pa[\"phase_2\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    \n",
    "    print(f\"第一阶段 (快速靠近) - 平均步数: {np.mean(phase_1_steps):.2f}, 平均加速边: {np.mean(phase_1_accel_edges):.2f}\")\n",
    "    print(f\"第二阶段 (Beam Search) - 平均步数: {np.mean(phase_2_steps):.2f}, 平均加速边: {np.mean(phase_2_accel_edges):.2f}\")\n",
    "    \n",
    "    # 计算加速边使用比例\n",
    "    total_accel_edges = [pa[\"total_accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    total_steps = [pa[\"total_steps\"] for pa in PHASE_ANALYSIS]\n",
    "    accel_edge_ratios = [accel/steps if steps > 0 else 0 for accel, steps in zip(total_accel_edges, total_steps)]\n",
    "    \n",
    "    print(f\"整体加速边使用比例: {np.mean(accel_edge_ratios):.2%}\")\n",
    "    \n",
    "    # 分析哪些查询受益最多\n",
    "    if len(PHASE_ANALYSIS) > 0:\n",
    "        best_benefit_idx = np.argmax(accel_edge_ratios)\n",
    "        best_benefit = PHASE_ANALYSIS[best_benefit_idx]\n",
    "        print(f\"\\n加速边受益最多的查询:\")\n",
    "        print(f\"  第一阶段: {best_benefit['phase_1']['step_count']} 步, {best_benefit['phase_1']['accel_edges']} 条加速边\")\n",
    "        print(f\"  第二阶段: {best_benefit['phase_2']['step_count']} 步, {best_benefit['phase_2']['accel_edges']} 条加速边\")\n",
    "        print(f\"  总步数: {best_benefit['total_steps']}, 总加速边: {best_benefit['total_accel_edges']}\")\n",
    "        print(f\"  加速边比例: {best_benefit['overall_accel_edge_ratio']:.2%}\")\n",
    "\n",
    "\n",
    "arr_ori_bak = np.array(NUM_STEPS, dtype=np.float64)\n",
    "arr_ori = arr_ori_bak.copy()\n",
    "arr_ori.sort()\n",
    "\n",
    "mean_steps = arr_ori.mean()\n",
    "P50_steps = np.percentile(arr_ori, 50)\n",
    "p99_steps = np.percentile(arr_ori, 99)\n",
    "print(f\"\\n原始搜索统计:\")\n",
    "print(f\"mean steps: {mean_steps}\")\n",
    "print(f\"middle steps: {P50_steps}\")\n",
    "print(f\"p99 steps: {p99_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 构建 RoarGraph 风格的 Cross Distribution 边 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross distribution 边构建统计:\n",
      "{'query_processed': True, 'layer_1_nodes_total': 3092, 'top_k_selected': 10, 'pairs_considered': 45, 'pairs_added': 10, 'skipped_existing': 35, 'pruned_by_cap': 0, 'edges_added': 10, 'top_k_nodes': [123656, 125677, 158436, 183136, 132312, 179175, 141094, 190128, 156261, 174419], 'query_distance': 0.3494441509246826}\n",
      "\n",
      "Cross distribution 边统计:\n",
      "总添加的 cross distribution 边: 1512998\n",
      "被删除的 cross distribution 边: 1374116\n",
      "活跃的 cross distribution 边: 138882\n"
     ]
    }
   ],
   "source": [
    "# 使用新的 RoarGraph 风格的 cross distribution 边构建\n",
    "print(\"\\n=== 构建 RoarGraph 风格的 Cross Distribution 边 ===\")\n",
    "for query in query_vector:\n",
    "    stats = index.build_cross_distribution_edges(\n",
    "        max_new_edges_per_node=4,\n",
    "        query=query,\n",
    "    )\n",
    "print(\"Cross distribution 边构建统计:\")\n",
    "print(stats)\n",
    "\n",
    "# 获取 cross distribution 边的统计信息\n",
    "cross_stats = index.get_cross_distribution_stats()\n",
    "print(\"\\nCross distribution 边统计:\")\n",
    "print(f\"总添加的 cross distribution 边: {cross_stats['total_cross_edges']}\")\n",
    "print(f\"被删除的 cross distribution 边: {cross_stats['deleted_cross_edges']}\")\n",
    "print(f\"活跃的 cross distribution 边: {cross_stats['active_cross_edges']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 阶段分析统计 ===\n",
      "第一阶段 (快速靠近) - 平均步数: 525.21, 平均加速边: 275.88\n",
      "第二阶段 (Beam Search) - 平均步数: 72.71, 平均加速边: 0.00\n",
      "整体加速边使用比例: 47.27%\n",
      "\n",
      "加速边受益最多的查询:\n",
      "  第一阶段: 372 步, 267 条加速边\n",
      "  第二阶段: 0 步, 0 条加速边\n",
      "  总步数: 372, 总加速边: 267\n",
      "  加速边比例: 71.77%\n",
      "\n",
      "原始搜索统计:\n",
      "mean steps: 597.9242759032547\n",
      "middle steps: 585.0\n",
      "p99 steps: 1040.0\n"
     ]
    }
   ],
   "source": [
    "# OOD search steps\n",
    "NUM_STEPS = []\n",
    "PHASE_ANALYSIS = []\n",
    "for qid, target_list in test_query_list.items():\n",
    "    q = query_vector[qid]\n",
    "    for target_id in target_list[:10]:\n",
    "        if target_id not in IMAGE_IDX_SET:\n",
    "            continue\n",
    "        # 使用阶段分析功能\n",
    "        out = index.search_steps_to_target(q, target_id, k=10, ef=64, analyze_phases=True, verbose=False, multi_path_search=True, max_paths=3)\n",
    "        NUM_STEPS.append(len(out[\"trace\"]))\n",
    "        if \"phase_analysis\" in out:\n",
    "            PHASE_ANALYSIS.append(out[\"phase_analysis\"])\n",
    "\n",
    "\n",
    "# 分析阶段统计\n",
    "if PHASE_ANALYSIS:\n",
    "    print(\"\\n=== 阶段分析统计 ===\")\n",
    "    phase_1_steps = [pa[\"phase_1\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_steps = [pa[\"phase_2\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_1_accel_edges = [pa[\"phase_1\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_accel_edges = [pa[\"phase_2\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    \n",
    "    print(f\"第一阶段 (快速靠近) - 平均步数: {np.mean(phase_1_steps):.2f}, 平均加速边: {np.mean(phase_1_accel_edges):.2f}\")\n",
    "    print(f\"第二阶段 (Beam Search) - 平均步数: {np.mean(phase_2_steps):.2f}, 平均加速边: {np.mean(phase_2_accel_edges):.2f}\")\n",
    "    \n",
    "    # 计算加速边使用比例\n",
    "    total_accel_edges = [pa[\"total_accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    total_steps = [pa[\"total_steps\"] for pa in PHASE_ANALYSIS]\n",
    "    accel_edge_ratios = [accel/steps if steps > 0 else 0 for accel, steps in zip(total_accel_edges, total_steps)]\n",
    "    \n",
    "    print(f\"整体加速边使用比例: {np.mean(accel_edge_ratios):.2%}\")\n",
    "    \n",
    "    # 分析哪些查询受益最多\n",
    "    if len(PHASE_ANALYSIS) > 0:\n",
    "        best_benefit_idx = np.argmax(accel_edge_ratios)\n",
    "        best_benefit = PHASE_ANALYSIS[best_benefit_idx]\n",
    "        print(f\"\\n加速边受益最多的查询:\")\n",
    "        print(f\"  第一阶段: {best_benefit['phase_1']['step_count']} 步, {best_benefit['phase_1']['accel_edges']} 条加速边\")\n",
    "        print(f\"  第二阶段: {best_benefit['phase_2']['step_count']} 步, {best_benefit['phase_2']['accel_edges']} 条加速边\")\n",
    "        print(f\"  总步数: {best_benefit['total_steps']}, 总加速边: {best_benefit['total_accel_edges']}\")\n",
    "        print(f\"  加速边比例: {best_benefit['overall_accel_edge_ratio']:.2%}\")\n",
    "\n",
    "\n",
    "arr_ori_bak = np.array(NUM_STEPS, dtype=np.float64)\n",
    "arr_ori = arr_ori_bak.copy()\n",
    "arr_ori.sort()\n",
    "\n",
    "mean_steps = arr_ori.mean()\n",
    "P50_steps = np.percentile(arr_ori, 50)\n",
    "p99_steps = np.percentile(arr_ori, 99)\n",
    "print(f\"\\n原始搜索统计:\")\n",
    "print(f\"mean steps: {mean_steps}\")\n",
    "print(f\"middle steps: {P50_steps}\")\n",
    "print(f\"p99 steps: {p99_steps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
