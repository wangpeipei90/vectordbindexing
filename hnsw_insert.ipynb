{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "from io_utils import read_fbin, read_ibin\n",
    "import faiss\n",
    "print(faiss.__version__)\n",
    "import numpy as np\n",
    "file_path = \"/root/code/vectordbindexing/Text2Image/base.1M.fbin\"\n",
    "query_path = \"/root/code/vectordbindexing/Text2Image/query.public.100K.fbin\"\n",
    "ground_truth_path = \"/root/code/vectordbindexing/Text2Image/groundtruth.public.100K.ibin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "reading image vector: ---\n",
      "<class 'numpy.ndarray'>\n",
      "2 (1000000, 200) float32 200000000\n",
      "\n",
      "\n",
      "reading querys: ---\n",
      "<class 'numpy.ndarray'>\n",
      "2 (100000, 200) float32 20000000\n"
     ]
    }
   ],
   "source": [
    "# read datasets\n",
    "print(\"\\n\\nreading image vector: ---\")\n",
    "data_vector = read_fbin(file_path)\n",
    "print(type(data_vector))\n",
    "print(data_vector.ndim, data_vector.shape, data_vector.dtype, data_vector.size)\n",
    "# print(data_vector[:1])  # Print first 1 elements to verify content\n",
    "\n",
    "# train_data_vector = data_vector[:500000]\n",
    "# insert_10_percent = data_vector[500000:550000]\n",
    "# insert_20_percent = data_vector[550000:600000]\n",
    "# insert_30_percent = data_vector[600000:650000]\n",
    "# insert_40_percent = data_vector[650000:700000]\n",
    "# insert_50_percent = data_vector[700000:750000]\n",
    "# insert_100_percent = data_vector[750000:]\n",
    "train_data_vector = data_vector[:500000]\n",
    "insert_1_percent = data_vector[500000:505000]\n",
    "insert_2_percent = data_vector[505000:510000]\n",
    "insert_3_percent = data_vector[510000:515000]\n",
    "insert_4_percent = data_vector[515000:520000]\n",
    "insert_5_percent = data_vector[520000:525000]\n",
    "insert_10_percent = data_vector[525000:550000]\n",
    "\n",
    "# read querys\n",
    "print(\"\\n\\nreading querys: ---\")\n",
    "query_vector = read_fbin(query_path)\n",
    "print(type(query_vector))\n",
    "print(query_vector.ndim, query_vector.shape, query_vector.dtype, query_vector.size)\n",
    "# print(query_vector[0])  # Print first 3 elements to verify content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hnsw_cosine\n",
    "import simple_sim_hash\n",
    "import importlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "importlib.reload(hnsw_cosine)\n",
    "\n",
    "index = hnsw_cosine.HNSWIndex(M=32, ef_construction=128, ef_search=64, random_seed=1)\n",
    "simHash = simple_sim_hash.SimpleSimHash(dim=200)\n",
    "\n",
    "IMAGE_IDX_SET = set()\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def insert_one(args):\n",
    "    img_id, vec = args\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            index.add_item_fast10k(vec, lsh=simHash, limit=100)\n",
    "            return img_id\n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(1)  # 等 1 秒再试\n",
    "            else:\n",
    "                # 达到最大重试次数仍失败\n",
    "                raise\n",
    "    return img_id  # 理论不会走到这里\n",
    "\n",
    "# 形状 [N,200]（先用1M子集或更小切片做原型）\n",
    "for img_id, vec in enumerate(train_data_vector):        # 可加 tqdm、批量 flush\n",
    "    index.add_item_fast10k(vec, lsh=simHash, limit=100)\n",
    "    IMAGE_IDX_SET.add(img_id)\n",
    "# with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "#     futures = [executor.submit(insert_one, (img_id, vec))\n",
    "#                for img_id, vec in enumerate(train_data_vector)]\n",
    "#     for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "#         try:\n",
    "#             img_id = future.result()\n",
    "#             IMAGE_IDX_SET.add(img_id)\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] Failed after retries: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train: 10000\n",
      "num of test: 2000\n"
     ]
    }
   ],
   "source": [
    "# 读取faiss搜索结果，获取 query_vector 和 search 结果\n",
    "import json\n",
    "train_query_list = {}\n",
    "test_query_list = {}\n",
    "with open(\"./search_results_1M_100.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    for query_idx, vec_list in data.items():\n",
    "        if int(query_idx) % 6 != 0:\n",
    "            train_query_list[int(query_idx)] = vec_list\n",
    "        else:\n",
    "            test_query_list[int(query_idx)] = vec_list\n",
    "print(f\"num of train: {len(train_query_list)}\")\n",
    "print(f\"num of test: {len(test_query_list)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset added links\n",
    "# stats = index.augment_from_query_topk(\n",
    "#     train_query_list,\n",
    "#     strategy=\"projection\",\n",
    "#     layer=0,\n",
    "#     max_new_edges_per_node=0,\n",
    "#     reset_ = True\n",
    "# )\n",
    "# print(stats)\n",
    "\n",
    "for layer in range(0, 100):\n",
    "    if layer not in index.neighbours:\n",
    "        continue\n",
    "    for idx in range(0, 500000):\n",
    "        if idx not in index.neighbours[layer]:\n",
    "            continue\n",
    "        index.neighbours[layer][idx] = index.neighbours[layer][idx][:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean steps: 418.1821557607387\n",
      "p99 steps: 798.9600000000064\n",
      "p995 steps: 861.0\n",
      "p999 steps: 900.0\n"
     ]
    }
   ],
   "source": [
    "# OOD search steps\n",
    "NUM_STEPS = []\n",
    "for qid, target_list in test_query_list.items():\n",
    "    q = query_vector[qid]\n",
    "    for target_id in target_list[:10]:\n",
    "        if target_id not in IMAGE_IDX_SET:\n",
    "            continue\n",
    "        # out = index.query(q, 10)\n",
    "        out = index.search_steps_to_target(q, target_id+1, k=10, ef=64)\n",
    "        NUM_STEPS.append(len(out[\"trace\"]))\n",
    "\n",
    "arr = np.array(NUM_STEPS, dtype=np.float64)\n",
    "arr.sort()\n",
    "\n",
    "mean_steps = arr.mean()\n",
    "p99_steps = np.percentile(arr, 99)\n",
    "p995_steps = np.percentile(arr, 99.5)\n",
    "p999_steps = np.percentile(arr, 99.9)\n",
    "print(f\"mean steps: {mean_steps}\")\n",
    "print(f\"p99 steps: {p99_steps}\")\n",
    "print(f\"p995 steps: {p995_steps}\")\n",
    "print(f\"p999 steps: {p999_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pairs_considered': 2519132, 'pairs_added': 39917, 'skipped_missing': 99122, 'skipped_existing': 598, 'pruned_by_cap': 2478617, 'skipped_occluded': 0}\n"
     ]
    }
   ],
   "source": [
    "# add links to the graph\n",
    "stats = index.augment_from_query_topk(\n",
    "    test_query_list,\n",
    "    strategy=\"clique\",\n",
    "    layer=0,\n",
    "    max_new_edges_per_node=1,\n",
    ")\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean steps: 537.1999801311346\n",
      "p99 steps: 945.0\n",
      "p995 steps: 987.0\n",
      "p999 steps: 1210.0\n"
     ]
    }
   ],
   "source": [
    "# OOD search steps - after add links\n",
    "NUM_STEPS = []\n",
    "for qid, target_list in test_query_list.items():\n",
    "    q = query_vector[qid]\n",
    "    for target_id in target_list[:10]:\n",
    "        if target_id not in IMAGE_IDX_SET:\n",
    "            continue\n",
    "        out = index.search_steps_to_target(q, target_id+1, k=10, ef=64)\n",
    "        NUM_STEPS.append(len(out[\"trace\"]))\n",
    "\n",
    "arr = np.array(NUM_STEPS, dtype=np.float64)\n",
    "arr.sort()\n",
    "\n",
    "mean_steps = arr.mean()\n",
    "p99_steps = np.percentile(arr, 99)\n",
    "p995_steps = np.percentile(arr, 99.5)\n",
    "p999_steps = np.percentile(arr, 99.9)\n",
    "print(f\"mean steps: {mean_steps}\")\n",
    "print(f\"p99 steps: {p99_steps}\")\n",
    "print(f\"p995 steps: {p995_steps}\")\n",
    "print(f\"p999 steps: {p999_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# out = index.query(q, 10)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_steps_to_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m         NUM_STEPS\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     28\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(NUM_STEPS, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "File \u001b[0;32m~/code/vectordbindexing/hnsw_cosine.py:513\u001b[0m, in \u001b[0;36mHNSWIndex.search_steps_to_target\u001b[0;34m(self, vector, target_id, k, ef)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# 3) Optionally finish a normal search to return a final top-k list\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# （这部分不影响“找到 target 的时刻”，仅用于返回参考的 top-k）\u001b[39;00m\n\u001b[1;32m    512\u001b[0m cands \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_layer(vec, curr, \u001b[38;5;241m0\u001b[39m, eff)\n\u001b[0;32m--> 513\u001b[0m dists \u001b[38;5;241m=\u001b[39m [(cid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance(vec, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[cid]\u001b[38;5;241m.\u001b[39mvector)) \u001b[38;5;28;01mfor\u001b[39;00m cid \u001b[38;5;129;01min\u001b[39;00m cands]\n\u001b[1;32m    514\u001b[0m dists\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    515\u001b[0m topk \u001b[38;5;241m=\u001b[39m dists[:k]\n",
      "File \u001b[0;32m~/code/vectordbindexing/hnsw_cosine.py:513\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# 3) Optionally finish a normal search to return a final top-k list\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# （这部分不影响“找到 target 的时刻”，仅用于返回参考的 top-k）\u001b[39;00m\n\u001b[1;32m    512\u001b[0m cands \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_layer(vec, curr, \u001b[38;5;241m0\u001b[39m, eff)\n\u001b[0;32m--> 513\u001b[0m dists \u001b[38;5;241m=\u001b[39m [(cid, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcid\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m cid \u001b[38;5;129;01min\u001b[39;00m cands]\n\u001b[1;32m    514\u001b[0m dists\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    515\u001b[0m topk \u001b[38;5;241m=\u001b[39m dists[:k]\n",
      "File \u001b[0;32m~/code/vectordbindexing/hnsw_cosine.py:14\u001b[0m, in \u001b[0;36mcosine_distance\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcosine_distance\u001b[39m(a: np\u001b[38;5;241m.\u001b[39mndarray, b: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# 先离线把向量做 unit-norm，再使用 1 - 内积（= 1 - cos）\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 插入额外的数据；并继续search上面的测试集合，查看search所需steps\n",
    "insert_data_vectors = {\n",
    "    \"insert_1%\": insert_1_percent,\n",
    "    \"insert_2%\": insert_2_percent,\n",
    "    \"insert_3%\": insert_3_percent,\n",
    "    \"insert_4%\": insert_4_percent,\n",
    "    \"insert_5%\": insert_5_percent,\n",
    "    \"insert_10%\": insert_10_percent,\n",
    "}\n",
    "img_id = 500000\n",
    "for name, insert_vectors in insert_data_vectors.items():\n",
    "    # insert 新节点\n",
    "    for _, vec in enumerate(insert_vectors):        # 可加 tqdm、批量 flush\n",
    "        index.add_item(vec, id=img_id)\n",
    "        img_id += 1\n",
    "        IMAGE_IDX_SET.add(img_id)\n",
    "    \n",
    "    NUM_STEPS = []\n",
    "    for qid, target_list in test_query_list.items():\n",
    "        q = query_vector[qid]\n",
    "        for target_id in target_list[:10]:\n",
    "            if target_id not in IMAGE_IDX_SET:\n",
    "                continue\n",
    "            # out = index.query(q, 10)\n",
    "            out = index.search_steps_to_target(q, target_id+1, k=10, ef=64)\n",
    "            NUM_STEPS.append(len(out[\"trace\"]))\n",
    "\n",
    "    arr = np.array(NUM_STEPS, dtype=np.float64)\n",
    "    arr.sort()\n",
    "\n",
    "    mean_steps = arr.mean()\n",
    "    p99_steps = np.percentile(arr, 99)\n",
    "    p995_steps = np.percentile(arr, 99.5)\n",
    "    p999_steps = np.percentile(arr, 99.9)\n",
    "    print(f\"mean steps: {mean_steps}\")\n",
    "    print(f\"p99 steps: {p99_steps}\")\n",
    "    print(f\"p995 steps: {p995_steps}\")\n",
    "    print(f\"p999 steps: {p999_steps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
