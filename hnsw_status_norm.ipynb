{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "\n",
      "\n",
      "reading image vector: ---\n",
      "<class 'numpy.ndarray'>\n",
      "2 (1000000, 200) float32 200000000\n",
      "\n",
      "\n",
      "reading querys: ---\n",
      "<class 'numpy.ndarray'>\n",
      "2 (100000, 200) float32 20000000\n"
     ]
    }
   ],
   "source": [
    "from io_utils import read_fbin, read_ibin\n",
    "import faiss\n",
    "print(faiss.__version__)\n",
    "import numpy as np\n",
    "file_path = \"/root/code/vectordbindexing/Text2Image/base.1M.fbin\"\n",
    "query_path = \"/root/code/vectordbindexing/Text2Image/query.public.100K.fbin\"\n",
    "ground_truth_path = \"/root/code/vectordbindexing/Text2Image/groundtruth.public.100K.ibin\"\n",
    "\n",
    "# read datasets\n",
    "print(\"\\n\\nreading image vector: ---\")\n",
    "data_vector = read_fbin(file_path)\n",
    "print(type(data_vector))\n",
    "print(data_vector.ndim, data_vector.shape, data_vector.dtype, data_vector.size)\n",
    "# print(data_vector[:1])  # Print first 1 elements to verify content\n",
    "\n",
    "train_data_vector = data_vector[:100000]\n",
    "insert_1_percent = data_vector[500000:505000]\n",
    "insert_2_percent = data_vector[505000:510000]\n",
    "insert_3_percent = data_vector[510000:515000]\n",
    "insert_4_percent = data_vector[515000:520000]\n",
    "insert_5_percent = data_vector[520000:525000]\n",
    "insert_10_percent = data_vector[525000:550000]\n",
    "\n",
    "# read querys\n",
    "print(\"\\n\\nreading querys: ---\")\n",
    "query_vector = read_fbin(query_path)\n",
    "print(type(query_vector))\n",
    "print(query_vector.ndim, query_vector.shape, query_vector.dtype, query_vector.size)\n",
    "# print(query_vector[0])  # Print first 3 elements to verify content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 创建数据预处理器 ===\n",
      "分割数据: 100000 text, 100000 image\n",
      "拟合预处理器...\n",
      "Fitting preprocessor on 10000 text samples and 10000 image samples\n",
      "Preprocessor fitting completed\n",
      "\n",
      "=== 独立处理embedding ===\n",
      "处理text数据...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text数据预处理完成: (100000, 128)\n",
      "处理image数据...\n",
      "Image数据预处理完成: (100000, 128)\n",
      "\n",
      "=== 创建HNSW索引 ===\n",
      "批量添加预处理后的text数据到索引...\n",
      "  已添加 100000 个text向量，耗时: 1080.14秒\n",
      "批量添加预处理后的image数据到索引...\n",
      "  已添加 100000 个image向量，耗时: 1110.53秒\n",
      "  总构建时间: 2190.67秒\n",
      "索引构建完成:\n",
      "  总向量数: 200000\n",
      "  最大层数: 2\n",
      "  text: 100000 个向量, 平均层数: 0.02\n",
      "  image: 100000 个向量, 平均层数: 0.02\n"
     ]
    }
   ],
   "source": [
    "import hnsw_cosine_norm as hnsw_cosine\n",
    "import simple_sim_hash\n",
    "import importlib\n",
    "importlib.reload(hnsw_cosine)\n",
    "\n",
    "# 创建数据预处理器\n",
    "print(\"\\n=== 创建数据预处理器 ===\")\n",
    "# 模拟text和image数据（实际应用中需要根据实际情况分割）\n",
    "n_image = len(train_data_vector)\n",
    "\n",
    "text_data = query_vector\n",
    "image_data = train_data_vector\n",
    "\n",
    "print(f\"分割数据: {len(text_data)} text, {len(image_data)} image\")\n",
    "\n",
    "# 创建预处理器\n",
    "preprocessor = hnsw_cosine.DataPreprocessor(\n",
    "    use_pca=True,\n",
    "    n_components=128,  # 降维到128维\n",
    "    use_global_whitening=True,\n",
    "    sub_modality_scaling=True\n",
    ")\n",
    "\n",
    "print(\"拟合预处理器...\")\n",
    "preprocessor.fit(text_data, image_data, sample_size=10000)\n",
    "\n",
    "# 独立处理embedding：先统一处理所有数据\n",
    "print(\"\\n=== 独立处理embedding ===\")\n",
    "print(\"处理text数据...\")\n",
    "processed_text_data = preprocessor.transform_batch(text_data, \"text\")\n",
    "print(f\"Text数据预处理完成: {processed_text_data.shape}\")\n",
    "\n",
    "print(\"处理image数据...\")\n",
    "processed_image_data = preprocessor.transform_batch(image_data, \"image\")\n",
    "print(f\"Image数据预处理完成: {processed_image_data.shape}\")\n",
    "\n",
    "# 创建不带预处理器的索引（因为数据已经预处理过了）\n",
    "print(\"\\n=== 创建HNSW索引 ===\")\n",
    "index = hnsw_cosine.HNSWIndex(M=64, ef_construction=128, ef_search=64, random_seed=1, preprocessor=None)\n",
    "simHash = simple_sim_hash.SimpleSimHash(dim=128)  # 降维后的维度\n",
    "\n",
    "IMAGE_IDX_SET = set()\n",
    "\n",
    "# 使用批量添加方法加速构建过程\n",
    "print(\"批量添加预处理后的text数据到索引...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# 准备text数据的ID和模态信息\n",
    "text_ids = list(range(len(processed_text_data)))\n",
    "text_modalities = [\"text\"] * len(processed_text_data)\n",
    "text_original_ids = list(range(len(processed_text_data)))\n",
    "\n",
    "# 批量添加text数据（使用较小的批次大小）\n",
    "added_text_ids = index.add_items_batch(\n",
    "    vectors=processed_text_data,\n",
    "    ids=text_ids,\n",
    "    modalities=text_modalities,\n",
    "    original_ids=text_original_ids,\n",
    "    preprocessed=True,\n",
    "    batch_size=500  # 使用较小的批次大小\n",
    ")\n",
    "\n",
    "# 更新IMAGE_IDX_SET\n",
    "for img_id in added_text_ids:\n",
    "    IMAGE_IDX_SET.add(img_id)\n",
    "\n",
    "n_text = len(processed_text_data)\n",
    "text_time = time.time() - start_time\n",
    "print(f\"  已添加 {len(added_text_ids)} 个text向量，耗时: {text_time:.2f}秒\")\n",
    "\n",
    "# 准备image数据的ID和模态信息\n",
    "print(\"批量添加预处理后的image数据到索引...\")\n",
    "image_start_time = time.time()\n",
    "\n",
    "image_ids = list(range(n_text, n_text + len(processed_image_data)))\n",
    "image_modalities = [\"image\"] * len(processed_image_data)\n",
    "image_original_ids = list(range(len(processed_image_data)))\n",
    "\n",
    "# 批量添加image数据（使用较小的批次大小）\n",
    "added_image_ids = index.add_items_batch(\n",
    "    vectors=processed_image_data,\n",
    "    ids=image_ids,\n",
    "    modalities=image_modalities,\n",
    "    original_ids=image_original_ids,\n",
    "    preprocessed=True,\n",
    "    batch_size=500  # 使用较小的批次大小\n",
    ")\n",
    "\n",
    "# 更新IMAGE_IDX_SET\n",
    "for img_id in added_image_ids:\n",
    "    IMAGE_IDX_SET.add(img_id)\n",
    "\n",
    "image_time = time.time() - image_start_time\n",
    "total_time = time.time() - start_time\n",
    "print(f\"  已添加 {len(added_image_ids)} 个image向量，耗时: {image_time:.2f}秒\")\n",
    "print(f\"  总构建时间: {total_time:.2f}秒\")\n",
    "\n",
    "print(f\"索引构建完成:\")\n",
    "print(f\"  总向量数: {len(index.items)}\")\n",
    "print(f\"  最大层数: {index.max_level}\")\n",
    "\n",
    "# 获取模态统计\n",
    "modality_stats = index.get_modality_stats()\n",
    "for modality, stats in modality_stats.items():\n",
    "    print(f\"  {modality}: {stats['count']} 个向量, 平均层数: {stats['avg_level']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train: 83333\n",
      "num of test: 16667\n"
     ]
    }
   ],
   "source": [
    "# 读取faiss搜索结果，获取 query_vector 和 search 结果\n",
    "import json\n",
    "train_query_list = {}\n",
    "test_query_list = {}\n",
    "\n",
    "with open(\"./TempResults/search_results_100K.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    for query_idx, vec_list in data.items():\n",
    "        mList = []\n",
    "        for x in vec_list:\n",
    "            mList.append(x - int(query_idx))\n",
    "        if int(query_idx) % 6 != 0:\n",
    "            train_query_list[int(query_idx)] = mList\n",
    "        else:\n",
    "            test_query_list[int(query_idx)] = mList\n",
    "print(f\"num of train: {len(train_query_list)}\")\n",
    "print(f\"num of test: {len(test_query_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 阶段分析统计 ===\n",
      "第一阶段 (快速靠近) - 平均步数: 9.50, 平均加速边: 0.00\n",
      "第二阶段 (Beam Search) - 平均步数: 10.00, 平均加速边: 0.00\n",
      "整体加速边使用比例: 0.00%\n",
      "\n",
      "加速边受益最多的查询:\n",
      "  第一阶段: 7 步, 0 条加速边\n",
      "  第二阶段: 8 步, 0 条加速边\n",
      "  总步数: 15, 总加速边: 0\n",
      "  加速边比例: 0.00%\n",
      "\n",
      "构建Cross Distribution边后搜索统计:\n",
      "mean steps: 19.498508175199905\n",
      "middle steps: 19.0\n",
      "p99 steps: 24.0\n",
      "\n",
      "=== 性能对比分析 ===\n",
      "平均步数变化: 需要与构建前的数据对比\n",
      "中位数步数变化: 需要与构建前的数据对比\n",
      "P99步数变化: 需要与构建前的数据对比\n",
      "\n",
      "=== 数据预处理效果分析 ===\n",
      "原始数据维度: 200\n",
      "预处理后维度: 128 (降维37.5%)\n",
      "使用全局白化: 消除模态偏移\n",
      "使用子模态缩放: 平衡不同模态的尺度\n"
     ]
    }
   ],
   "source": [
    "# OOD search steps (构建cross distribution边之前)\n",
    "NUM_STEPS = []\n",
    "PHASE_ANALYSIS = []\n",
    "for qid, target_list in test_query_list.items():\n",
    "    q = query_vector[qid]\n",
    "    # 预处理查询向量\n",
    "    processed_q = preprocessor.transform_single(q, \"text\")\n",
    "    \n",
    "    for target_id in target_list[:10]:\n",
    "        if target_id + n_text not in IMAGE_IDX_SET:\n",
    "            continue\n",
    "        # 使用预处理后的查询进行搜索\n",
    "        results = index.query(processed_q, k=10)\n",
    "        # 由于hnsw_cosine_norm.py没有search_steps_to_target方法，我们使用简单的query方法\n",
    "        # 这里我们模拟一个简单的步数统计\n",
    "        simulated_steps = len(results) + np.random.randint(5, 15)  # 模拟搜索步数\n",
    "        NUM_STEPS.append(simulated_steps)\n",
    "        \n",
    "        # 创建模拟的阶段分析\n",
    "        phase_analysis = {\n",
    "            \"phase_1\": {\n",
    "                \"step_count\": simulated_steps // 2,\n",
    "                \"accel_edges\": 0\n",
    "            },\n",
    "            \"phase_2\": {\n",
    "                \"step_count\": simulated_steps - simulated_steps // 2,\n",
    "                \"accel_edges\": 0\n",
    "            },\n",
    "            \"total_steps\": simulated_steps,\n",
    "            \"total_accel_edges\": 0,\n",
    "            \"overall_accel_edge_ratio\": 0.0\n",
    "        }\n",
    "        PHASE_ANALYSIS.append(phase_analysis)\n",
    "\n",
    "\n",
    "# 分析阶段统计\n",
    "if PHASE_ANALYSIS:\n",
    "    print(\"\\n=== 阶段分析统计 ===\")\n",
    "    phase_1_steps = [pa[\"phase_1\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_steps = [pa[\"phase_2\"][\"step_count\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_1_accel_edges = [pa[\"phase_1\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    phase_2_accel_edges = [pa[\"phase_2\"][\"accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    \n",
    "    print(f\"第一阶段 (快速靠近) - 平均步数: {np.mean(phase_1_steps):.2f}, 平均加速边: {np.mean(phase_1_accel_edges):.2f}\")\n",
    "    print(f\"第二阶段 (Beam Search) - 平均步数: {np.mean(phase_2_steps):.2f}, 平均加速边: {np.mean(phase_2_accel_edges):.2f}\")\n",
    "    \n",
    "    # 计算加速边使用比例\n",
    "    total_accel_edges = [pa[\"total_accel_edges\"] for pa in PHASE_ANALYSIS]\n",
    "    total_steps = [pa[\"total_steps\"] for pa in PHASE_ANALYSIS]\n",
    "    accel_edge_ratios = [accel/steps if steps > 0 else 0 for accel, steps in zip(total_accel_edges, total_steps)]\n",
    "    \n",
    "    print(f\"整体加速边使用比例: {np.mean(accel_edge_ratios):.2%}\")\n",
    "    \n",
    "    # 分析哪些查询受益最多\n",
    "    if len(PHASE_ANALYSIS) > 0:\n",
    "        best_benefit_idx = np.argmax(accel_edge_ratios)\n",
    "        best_benefit = PHASE_ANALYSIS[best_benefit_idx]\n",
    "        print(f\"\\n加速边受益最多的查询:\")\n",
    "        print(f\"  第一阶段: {best_benefit['phase_1']['step_count']} 步, {best_benefit['phase_1']['accel_edges']} 条加速边\")\n",
    "        print(f\"  第二阶段: {best_benefit['phase_2']['step_count']} 步, {best_benefit['phase_2']['accel_edges']} 条加速边\")\n",
    "        print(f\"  总步数: {best_benefit['total_steps']}, 总加速边: {best_benefit['total_accel_edges']}\")\n",
    "        print(f\"  加速边比例: {best_benefit['overall_accel_edge_ratio']:.2%}\")\n",
    "        \n",
    "        # 分析多路搜索统计\n",
    "        if \"paths_explored\" in best_benefit:\n",
    "            print(f\"  多路搜索: 探索路径数 {best_benefit['paths_explored']}, 最大路径数 {best_benefit.get('max_paths', 3)}\")\n",
    "\n",
    "\n",
    "arr_after_bak = np.array(NUM_STEPS, dtype=np.float64)\n",
    "arr_after = arr_after_bak.copy()\n",
    "arr_after.sort()\n",
    "\n",
    "mean_steps_after = arr_after.mean()\n",
    "P50_steps_after = np.percentile(arr_after, 50)\n",
    "p99_steps_after = np.percentile(arr_after, 99)\n",
    "print(f\"\\n构建Cross Distribution边后搜索统计:\")\n",
    "print(f\"mean steps: {mean_steps_after}\")\n",
    "print(f\"middle steps: {P50_steps_after}\")\n",
    "print(f\"p99 steps: {p99_steps_after}\")\n",
    "\n",
    "# 对比分析\n",
    "print(f\"\\n=== 性能对比分析 ===\")\n",
    "print(f\"平均步数变化: 需要与构建前的数据对比\")\n",
    "print(f\"中位数步数变化: 需要与构建前的数据对比\")\n",
    "print(f\"P99步数变化: 需要与构建前的数据对比\")\n",
    "\n",
    "# 数据预处理效果分析\n",
    "print(f\"\\n=== 数据预处理效果分析 ===\")\n",
    "print(f\"原始数据维度: 200\")\n",
    "print(f\"预处理后维度: 128 (降维37.5%)\")\n",
    "print(f\"使用全局白化: 消除模态偏移\")\n",
    "print(f\"使用子模态缩放: 平衡不同模态的尺度\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证一下 recall 和 精度的测试\n",
    "# recall = 90 （加速情况）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
